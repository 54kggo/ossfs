{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Ossfs API Reference","title":"Welcome to Ossfs"},{"location":"#welcome-to-ossfs","text":"API Reference","title":"Welcome to Ossfs"},{"location":"reference/ossfs/","text":"OSSFS A pythonic file-systems interface to OSS (Object Storage Service) AioOSSFileSystem Bases: BaseOSSFileSystem , AsyncFileSystem A pythonic file-systems interface to OSS (Object Storage Service) Base on async operations. Examples ossfs = AioOSSFileSystem(anon=False) ossfs.ls('my-bucket/') ['my-file.txt'] with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 class AioOSSFileSystem ( BaseOSSFileSystem , AsyncFileSystem ): # pylint: disable=abstract-method \"\"\" A pythonic file-systems interface to OSS (Object Storage Service) Base on async operations. Examples -------- >>> ossfs = AioOSSFileSystem(anon=False) >>> ossfs.ls('my-bucket/') ['my-file.txt'] >>> with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' \"\"\" # pylint:disable=no-value-for-parameter protocol = \"oss\" def __init__ ( self , psize : int = DEFAULT_POOL_SIZE , ** kwargs , ): \"\"\" ---------------------------------------------------------------- Addition arguments Args: psize (int, optional): concurrency number of the connections to the server. Defaults to DEFAULT_POOL_SIZE. \"\"\" super () . __init__ ( ** kwargs ) self . _psize = psize self . _session : Optional [ \"AioSession\" ] = None __init__ . __doc__ = ( BaseOSSFileSystem . __init__ . __doc__ + __init__ . __doc__ # type: ignore ) def _get_bucket ( self , bucket_name : str , connect_timeout : Optional [ int ] = None ) -> AioBucket : \"\"\" get the new aio bucket instance \"\"\" if self . _endpoint is None : raise ValueError ( \"endpoint is required\" ) try : return AioBucket ( auth = self . _auth , endpoint = self . _endpoint , bucket_name = bucket_name , connect_timeout = connect_timeout , session = self . _session , app_name = \"ossfs\" , ) except ClientError as err : raise FileNotFoundError ( bucket_name ) from err async def set_session ( self , refresh : bool = False ): \"\"\"Establish a connection session object. Returns ------- Session to be closed later with await .close() \"\"\" logger . debug ( \"Connect AioSession instance\" ) if self . _session is None or self . _session . closed or refresh : if self . _session is None : self . _session = AioSession ( self . _psize ) await self . _session . __aenter__ () # pylint: disable=unnecessary-dunder-call # the following actually closes the aiohttp connection; use of privates # might break in the future, would cause exception at gc time if not self . asynchronous : weakref . finalize ( self , self . close_session ) return def close_session ( self ): \"\"\"Close a connection session object.\"\"\" if self . _session is None or self . _session . closed : return if self . loop is not None and self . loop . is_running (): try : sync ( self . loop , self . _session . close , timeout = 0.1 ) return except FSTimeoutError : pass async def _call_oss ( self , method_name : str , * args , bucket : Optional [ str ] = None , timeout : Optional [ int ] = None , ** kwargs , ): if self . _endpoint is None : raise ValueError ( \"endpoint is required\" ) await self . set_session () if bucket : service : Union [ AioService , AioBucket ] = self . _get_bucket ( bucket , timeout ) else : service = AioService ( auth = self . _auth , endpoint = self . _endpoint , session = self . _session , connect_timeout = timeout , app_name = \"ossfs\" , ) method = getattr ( service , method_name , None ) try : if not method : method = getattr ( aiooss2 , method_name ) logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( service , * args , ** kwargs ) else : logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = await method ( * args , ** kwargs ) return out except ( RequestError , OssError ) as err : error = err raise translate_oss_error ( error ) from error async def _ls_dir ( # pylint: disable=too-many-arguments self , path : str , refresh : bool = False , max_items : int = 100 , delimiter : str = \"/\" , prefix : str = \"\" , ): norm_path = path . strip ( \"/\" ) if norm_path in self . dircache and not refresh and not prefix and delimiter : return self . dircache [ norm_path ] logger . debug ( \"Get directory listing for %s \" , norm_path ) bucket , key = self . split_path ( norm_path ) prefix = prefix or \"\" if key : prefix = f \" { key } / { prefix } \" files = [] async for obj_dict in self . _iterdir ( bucket , max_keys = max_items , delimiter = delimiter , prefix = prefix , ): files . append ( obj_dict ) if not prefix and delimiter == \"/\" : self . dircache [ norm_path ] = files return files async def _iterdir ( self , bucket : str , max_keys : int = 100 , delimiter : str = \"/\" , prefix : str = \"\" , ): \"\"\"Iterate asynchronously over files and directories under `prefix`. The contents are yielded in arbitrary order as info dicts. \"\"\" response = await self . _call_oss ( \"AioObjectIterator\" , bucket = bucket , prefix = prefix , delimiter = delimiter , max_keys = max_keys , ) obj : \"SimplifiedObjectInfo\" async for obj in response : data = self . _transfer_object_info_to_dict ( bucket , obj ) yield data async def _ls_buckets ( self , refresh : bool = False ) -> List [ Dict [ str , Any ]]: if \"\" not in self . dircache or refresh : if isinstance ( self . _auth , AnonymousAuth ): logging . warning ( \"cannot list buckets if not logged in\" ) return [] results : List [ Dict [ str , Any ]] = [] try : files : \"ListBucketsResult\" = await self . _call_oss ( \"list_buckets\" ) except ClientError : # listbucket permission missing return [] file : \"SimplifiedBucketInfo\" for file in files . buckets : data : Dict [ str , Any ] = {} data [ \"name\" ] = file . name data [ \"size\" ] = 0 data [ \"type\" ] = \"directory\" results . append ( data ) self . dircache [ \"\" ] = results else : results = self . dircache [ \"\" ] return results @async_prettify_info_result async def _ls ( self , path : str , detail : bool = True , ** kwargs ): \"\"\"List files in given bucket, or list of buckets. Listing is cached unless `refresh=True`. Note: only your buckets associated with the login will be listed by `ls('')`, not any public buckets (even if already accessed). Parameters ---------- path : string/bytes location at which to list files refresh : bool (=False) if False, look in local cache for file details first \"\"\" refresh = kwargs . pop ( \"refresh\" , False ) norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) if norm_path != \"\" : files = await self . _ls_dir ( path , refresh ) if not files and \"/\" in norm_path : files = await self . _ls_dir ( self . _parent ( path ), refresh = refresh ) files = [ file for file in files if file [ \"name\" ] . strip ( \"/\" ) == norm_path and file [ \"type\" ] != \"directory\" ] else : files = await self . _ls_buckets ( refresh ) return files @async_prettify_info_result async def _info ( self , path : str , ** kwargs ): norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : result = { \"name\" : path , \"size\" : 0 , \"type\" : \"directory\" } return result bucket , key = self . split_path ( norm_path ) self . _get_bucket ( bucket ) refresh = kwargs . pop ( \"refresh\" , False ) if not refresh : out = self . _ls_from_cache ( norm_path ) if out is not None : out = [ o for o in out if o [ \"name\" ] . strip ( \"/\" ) == norm_path ] if out : result = out [ 0 ] else : result = { \"name\" : norm_path , \"size\" : 0 , \"type\" : \"directory\" } return result if key : try : obj_out : \"HeadObjectResult\" = await self . _call_oss ( \"head_object\" , key , bucket = bucket , ) result = { \"LastModified\" : obj_out . last_modified , \"size\" : obj_out . content_length , \"name\" : path , \"type\" : \"file\" , } return result except ( PermissionError , FileNotFoundError ): pass # We check to see if the path is a directory by attempting to list its # contexts. If anything is found, it is indeed a directory try : ls_out = await self . _call_oss ( \"AioObjectIterator\" , bucket = bucket , prefix = key . rstrip ( \"/\" ) + \"/\" , delimiter = \"/\" , max_keys = 100 , ) try : async for _ in ls_out : return { \"size\" : 0 , \"name\" : path , \"type\" : \"directory\" , } except OssError as err : raise translate_oss_error ( err ) from err except ( PermissionError , FileNotFoundError ): pass else : for bucket_info in await self . _ls_buckets (): if bucket_info [ \"name\" ] == norm_path . rstrip ( \"/\" ): return { \"size\" : 0 , \"name\" : path , \"type\" : \"directory\" , } raise FileNotFoundError ( path ) def _cache_result_analysis ( self , norm_path : str , parent : str ) -> bool : if norm_path in self . dircache : for file_info in self . dircache [ norm_path ]: # For files the dircache can contain itself. # If it contains anything other than itself it is a directory. if file_info [ \"name\" ] != norm_path : return True return False for file_info in self . dircache [ parent ]: if file_info [ \"name\" ] == norm_path : # If we find ourselves return whether we are a directory return file_info [ \"type\" ] == \"directory\" return False async def _isdir ( self , path : str ) -> bool : norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) # Send buckets to super if norm_path == \"\" : return True if \"/\" not in norm_path : for bucket_info in await self . _ls_buckets (): if bucket_info [ \"name\" ] == norm_path : return True return False parent = self . _parent ( norm_path ) if norm_path in self . dircache or parent in self . dircache : return self . _cache_result_analysis ( norm_path , parent ) # This only returns things within the path and NOT the path object itself try : return bool ( await self . _ls_dir ( norm_path )) except FileNotFoundError : return False async def _put_file ( self , lpath : str , rpath : str , ** kwargs ): bucket , key = self . split_path ( rpath ) if os . path . isdir ( lpath ): if key : # don't make remote \"directory\" return await self . _mkdir ( lpath ) else : callback = as_progress_handler ( kwargs . get ( \"callback\" , None )) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : await self . _call_oss ( \"resumable_upload\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) else : await self . _call_oss ( \"put_object_from_file\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) self . invalidate_cache ( self . _parent ( rpath )) async def _get_file ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote file to local \"\"\" bucket , key = self . split_path ( rpath ) if await self . _isdir ( rpath ): # don't make local \"directory\" return callback = as_progress_handler ( kwargs . get ( \"callback\" , None )) if await self . _size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : await self . _call_oss ( \"resumable_download\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) else : await self . _call_oss ( \"get_object_to_file\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ** kwargs , ) @async_prettify_info_result async def _find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , # pylint: disable=unused-argument ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. prefix: str Only return files that match ``^{path}/{prefix}`` (if there is an exact match ``filename == {path}/{prefix}``, it also will be included) \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : for info in await self . _ls_dir ( path , delimiter = \"\" , prefix = prefix ): out . update ({ info [ \"name\" ]: info }) else : async for _ , dirs , files in self . _walk ( path , maxdepth , detail = True ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for _ , info in files . items ()}) if await self . _isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names } async def _bulk_delete ( self , pathlist , ** kwargs ): \"\"\" Remove multiple keys with one call Parameters ---------- pathlist : list(str) The keys to remove, must all be in the same bucket. Must have 0 < len <= 1000 \"\"\" if not pathlist : return bucket , key_list = self . _get_batch_delete_key_list ( pathlist ) await self . _call_oss ( \"batch_delete_objects\" , key_list , bucket = bucket ) async def _rm_file ( self , path : str , ** kwargs ): bucket , key = self . split_path ( path ) await self . _call_oss ( \"delete_object\" , bucket = bucket , key = key ) self . invalidate_cache ( self . _parent ( path )) async def _rm ( self , path , recursive = False , batch_size = 1000 , ** kwargs ): if isinstance ( path , list ): for file in path : await self . _rm ( file ) return paths = await self . _expand_path ( path , recursive = recursive ) await _run_coros_in_chunks ( [ self . _bulk_delete ( paths [ i : i + batch_size ]) for i in range ( 0 , len ( paths ), batch_size ) ], batch_size = 3 , nofiles = True , ) async def _checksum ( self , path , refresh = True ): \"\"\" Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. Parameters ---------- path : string/bytes path of file to get checksum for refresh : bool (=False) if False, look in local cache for file details first \"\"\" return sha256 ( ( str ( await self . _ukey ( path )) + str ( await self . _info ( path , refresh = refresh )) ) . encode () ) . hexdigest () checksum = sync_wrapper ( _checksum ) async def _ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = await self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc checksum = sync_wrapper ( _checksum ) async def _cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\"Copy file between locations on OSS. preserve_etag: bool Whether to preserve etag while copying. If the file is uploaded as a single part, then it will be always equalivent to the md5 hash of the file hence etag will always be preserved. But if the file is uploaded in multi parts, then this option will try to reproduce the same multipart upload while copying and preserve the generated etag. \"\"\" bucket2 , key2 = self . split_path ( path2 ) bucket1 , key1 = self . split_path ( path1 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket1 != bucket2 : tempdir = \".\" + self . ukey ( path1 ) await self . _get_file ( path1 , tempdir , ** kwargs ) await self . _put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) await self . _call_oss ( \"copy_object\" , bucket1 , key1 , key2 , bucket = bucket1 , timeout = connect_timeout , ) async def _append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket , key = self . split_path ( path ) result : \"AppendObjectResult\" = await self . _call_oss ( \"append_object\" , key , location , value , bucket = bucket , ) return result . next_position append_object = sync_wrapper ( _append_object ) async def _get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket , key = self . split_path ( path ) object_stream : \"AioGetObjectResult\" = await self . _call_oss ( \"get_object\" , key , bucket = bucket , byte_range = ( start , end ), headers = headers , ) results = b \"\" while True : result = await object_stream . read () if result : results += result else : break return results get_object = sync_wrapper ( _get_object ) async def _pipe_file ( self , path : str , value : Union [ str , bytes ], ** kwargs ): bucket , key = self . split_path ( path ) self . invalidate_cache ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): await self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return init_multi_part_upload_result : \"InitMultipartUploadResult\" = ( await self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): part_number = i + 1 value_block = value [ off : off + block_size ] put_object_result : \"PutObjectResult\" = await self . _call_oss ( \"upload_part\" , key , init_multi_part_upload_result . upload_id , part_number , value_block , bucket = bucket , ) parts . append ( PartInfo ( part_number , put_object_result . etag , size = len ( value_block ), part_crc = put_object_result . crc , ) ) await self . _call_oss ( \"complete_multipart_upload\" , key , init_multi_part_upload_result . upload_id , parts , bucket = bucket , ) async def _cat_file ( self , path : str , start = None , end = None , ** kwargs ): bucket , key = self . split_path ( path ) object_stream : \"AioGetObjectResult\" = await self . _call_oss ( \"get_object\" , bucket = bucket , key = key , byte_range = ( start , end ), ** kwargs , ) results = b \"\" while True : result = await object_stream . read () if not result : break results += result return results async def _modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or await self . _isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) object_meta = await self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( object_meta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () ) modified = sync_wrapper ( _modified ) __init__ ( psize = DEFAULT_POOL_SIZE , kwargs ) Addition arguments Parameters: Name Type Description Default psize int concurrency number of the connections to DEFAULT_POOL_SIZE Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , psize : int = DEFAULT_POOL_SIZE , ** kwargs , ): \"\"\" ---------------------------------------------------------------- Addition arguments Args: psize (int, optional): concurrency number of the connections to the server. Defaults to DEFAULT_POOL_SIZE. \"\"\" super () . __init__ ( ** kwargs ) self . _psize = psize self . _session : Optional [ \"AioSession\" ] = None close_session () Close a connection session object. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 122 123 124 125 126 127 128 129 130 131 def close_session ( self ): \"\"\"Close a connection session object.\"\"\" if self . _session is None or self . _session . closed : return if self . loop is not None and self . loop . is_running (): try : sync ( self . loop , self . _session . close , timeout = 0.1 ) return except FSTimeoutError : pass set_session ( refresh = False ) async Establish a connection session object. Returns Session to be closed later with await .close() Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 async def set_session ( self , refresh : bool = False ): \"\"\"Establish a connection session object. Returns ------- Session to be closed later with await .close() \"\"\" logger . debug ( \"Connect AioSession instance\" ) if self . _session is None or self . _session . closed or refresh : if self . _session is None : self . _session = AioSession ( self . _psize ) await self . _session . __aenter__ () # pylint: disable=unnecessary-dunder-call # the following actually closes the aiohttp connection; use of privates # might break in the future, would cause exception at gc time if not self . asynchronous : weakref . finalize ( self , self . close_session ) return OSSFile Bases: AbstractBufferedFile A file living in OSSFileSystem Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/file.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class OSSFile ( AbstractBufferedFile ): \"\"\"A file living in OSSFileSystem\"\"\" fs : Union [ \"OSSFileSystem\" , \"AioOSSFileSystem\" ] loc : int def _upload_chunk ( self , final : bool = False ) -> bool : \"\"\"Write one part of a multi-block file upload Parameters ========== final: bool This is the last block, so should complete file, if self.autocommit is True. \"\"\" self . loc = self . fs . append_object ( self . path , self . loc , self . buffer . getvalue ()) return True def _initiate_upload ( self ): \"\"\"Create remote file/upload\"\"\" if \"a\" in self . mode : self . loc = 0 if self . fs . exists ( self . path ): self . loc = self . fs . info ( self . path )[ \"size\" ] elif \"w\" in self . mode : # create empty file to append to self . loc = 0 if self . fs . exists ( self . path ): self . fs . rm_file ( self . path ) def _fetch_range ( self , start : int , end : int ) -> bytes : \"\"\" Get the specified set of bytes from remote Parameters ========== start: int end: int \"\"\" start = max ( start , 0 ) end = min ( self . size , end ) if start >= end or start >= self . size : return b \"\" return self . fs . get_object ( self . path , start , end ) OSSFileSystem Bases: BaseOSSFileSystem A pythonic file-systems interface to OSS (Object Storage Service) Examples ossfs = OSSFileSystem(anon=False) ossfs.ls('my-bucket/') ['my-file.txt'] with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 class OSSFileSystem ( BaseOSSFileSystem ): # pylint:disable=too-many-public-methods # pylint:disable=no-value-for-parameter \"\"\" A pythonic file-systems interface to OSS (Object Storage Service) Examples -------- >>> ossfs = OSSFileSystem(anon=False) >>> ossfs.ls('my-bucket/') ['my-file.txt'] >>> with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _session = oss2 . Session () def _get_bucket ( self , bucket_name : str , connect_timeout : Optional [ int ] = None ) -> oss2 . Bucket : \"\"\" get the new bucket instance \"\"\" if not self . _endpoint : raise ValueError ( \"endpoint is required\" ) try : return oss2 . Bucket ( self . _auth , self . _endpoint , bucket_name , session = self . _session , connect_timeout = connect_timeout , app_name = \"ossfs\" , ) except oss2 . exceptions . ClientError as err : raise FileNotFoundError ( bucket_name ) from err def _call_oss ( self , method_name : str , * args , bucket : Optional [ str ] = None , timeout : Optional [ int ] = None , retry : int = 3 , ** kwargs , ): if bucket : service = self . _get_bucket ( bucket , timeout ) else : service = oss2 . Service ( self . _auth , endpoint = self . _endpoint , connect_timeout = timeout , ) for count in range ( retry ): try : method = getattr ( service , method_name , None ) if not method : method = getattr ( oss2 , method_name ) logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( service , * args , ** kwargs ) else : logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( * args , ** kwargs ) return out except oss2 . exceptions . RequestError as err : logger . debug ( \"Retryable error: %s , try %s times\" , err , count + 1 ) error = err except oss2 . exceptions . OssError as err : logger . debug ( \"Nonretryable error: %s \" , err ) error = err break raise translate_oss_error ( error ) from error def _ls_bucket ( self , connect_timeout : Optional [ int ]) -> List [ Dict [ str , Any ]]: if \"\" not in self . dircache : results : List [ Dict [ str , Any ]] = [] if isinstance ( self . _auth , AnonymousAuth ): logging . warning ( \"cannot list buckets if not logged in\" ) return [] try : for bucket in self . _call_oss ( \"BucketIterator\" , timeout = connect_timeout ): result = { \"name\" : bucket . name , \"type\" : \"directory\" , \"size\" : 0 , \"CreateTime\" : bucket . creation_date , } results . append ( result ) except oss2 . exceptions . ClientError : pass self . dircache [ \"\" ] = copy . deepcopy ( results ) else : results = self . dircache [ \"\" ] return results def _get_object_info_list ( self , bucket_name : str , prefix : str , delimiter : str , connect_timeout : Optional [ int ], ): \"\"\" Wrap oss2.ObjectIterator return values into a fsspec form of file info \"\"\" result = [] obj : \"SimplifiedObjectInfo\" for obj in self . _call_oss ( \"ObjectIterator\" , prefix = prefix , delimiter = delimiter , bucket = bucket_name , timeout = connect_timeout , ): data = self . _transfer_object_info_to_dict ( bucket_name , obj ) result . append ( data ) return result def _ls_dir ( self , path : str , delimiter : str = \"/\" , refresh : bool = False , prefix : str = \"\" , connect_timeout : Optional [ int ] = None , ** kwargs , # pylint: disable=too-many-arguments ) -> List [ Dict ]: norm_path = path . strip ( \"/\" ) if norm_path in self . dircache and not refresh and not prefix and delimiter : return self . dircache [ norm_path ] logger . debug ( \"Get directory listing page for %s \" , norm_path ) bucket_name , key = self . split_path ( norm_path ) if not delimiter or prefix : if key : prefix = f \" { key } / { prefix } \" else : if norm_path in self . dircache and not refresh : return self . dircache [ norm_path ] if key : prefix = f \" { key } /\" try : self . dircache [ norm_path ] = self . _get_object_info_list ( bucket_name , prefix , delimiter , connect_timeout ) return self . dircache [ norm_path ] except oss2 . exceptions . AccessDenied : return [] @prettify_info_result def ls ( self , path : str , detail : bool = True , ** kwargs ): connect_timeout = kwargs . pop ( \"connect_timeout\" , 60 ) norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) if norm_path == \"\" : return self . _ls_bucket ( connect_timeout ) files = self . _ls_dir ( path , connect_timeout = connect_timeout ) if not files and \"/\" in norm_path : files = self . _ls_dir ( self . _parent ( path ), connect_timeout = connect_timeout ) files = [ file for file in files if file [ \"type\" ] != \"directory\" and file [ \"name\" ] . strip ( \"/\" ) == norm_path ] return files @prettify_info_result def find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ``ls``. \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : connect_timeout = kwargs . get ( \"connect_timeout\" , None ) for info in self . _ls_dir ( path , delimiter = \"\" , prefix = prefix , connect_timeout = connect_timeout ): out . update ({ info [ \"name\" ]: info }) else : for _ , dirs , files in self . walk ( path , maxdepth , detail = True , ** kwargs ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for name , info in files . items ()}) if self . isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names } def _directory_exists ( self , dirname : str , ** kwargs ): connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) ls_result = self . _ls_dir ( dirname , connect_timeout = connect_timeout ) return bool ( ls_result ) def _bucket_exist ( self , bucket_name : str ): if not bucket_name : return False try : self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) except ( oss2 . exceptions . OssError , PermissionError ): return False return True def exists ( self , path : str , ** kwargs ) -> bool : \"\"\"Is there a file at the given path\"\"\" norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : return True bucket_name , obj_name = self . split_path ( path ) if not self . _bucket_exist ( bucket_name ): return False connect_timeout = kwargs . get ( \"connect_timeout\" , None ) if not obj_name : return True if self . _call_oss ( \"object_exists\" , obj_name , bucket = bucket_name , timeout = connect_timeout , ): return True return self . _directory_exists ( path , ** kwargs ) def ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc def checksum ( self , path : str ): \"\"\"Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) \"\"\" return sha256 ( ( str ( self . ukey ( path )) + str ( self . info ( path ))) . encode () ) . hexdigest () def cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\" Copy within two locations in the filesystem # todo: big file optimization \"\"\" bucket_name1 , obj_name1 = self . split_path ( path1 ) bucket_name2 , obj_name2 = self . split_path ( path2 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket_name1 != bucket_name2 : tempdir = \".\" + self . ukey ( path1 ) self . get_file ( path1 , tempdir , ** kwargs ) self . put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) self . _call_oss ( \"copy_object\" , bucket_name1 , obj_name1 , obj_name2 , bucket = bucket_name1 , timeout = connect_timeout , ) def _rm ( self , path : Union [ str , List [ str ]]): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. \"\"\" if isinstance ( path , list ): for file in path : self . _rm ( file ) return bucket_name , obj_name = self . split_path ( path ) self . invalidate_cache ( self . _parent ( path )) self . _call_oss ( \"delete_object\" , obj_name , bucket = bucket_name ) def _bulk_delete ( self , pathlist , ** kwargs ): \"\"\" Remove multiple keys with one call Parameters ---------- pathlist : list(str) The keys to remove, must all be in the same bucket. Must have 0 < len <= 1000 \"\"\" if not pathlist : return bucket , key_list = self . _get_batch_delete_key_list ( pathlist ) self . _call_oss ( \"batch_delete_objects\" , key_list , bucket = bucket ) def rm ( self , path : Union [ str , List [ str ]], recursive = False , maxdepth = None ): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. recursive: bool If file(s) are directories, recursively delete contents and then also remove the directory maxdepth: int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. \"\"\" if isinstance ( path , list ): for file in path : self . rm ( file ) return path_expand = self . expand_path ( path , recursive = recursive , maxdepth = maxdepth ) def chunks ( lst : list , num : int ): for i in range ( 0 , len ( lst ), num ): yield lst [ i : i + num ] for files in chunks ( path_expand , 1000 ): self . _bulk_delete ( files ) def get_path ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote path to local \"\"\" if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) else : self . get_file ( rpath , lpath , ** kwargs ) def get_file ( self , rpath : str , lpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single remote file to local \"\"\" bucket_name , obj_name = self . split_path ( rpath ) kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) return connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if self . size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_download ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"get_object_to_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) def put_file ( self , lpath : str , rpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single file to remote \"\"\" kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) bucket_name , obj_name = self . split_path ( rpath ) if os . path . isdir ( lpath ): if obj_name : # don't make remote \"directory\" return self . mkdir ( lpath ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_upload ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"put_object_from_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) self . invalidate_cache ( self . _parent ( rpath )) def created ( self , path : str ): \"\"\"Return the created timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if obj_name : raise NotImplementedError ( \"OSS has no created timestamp\" ) bucket_info = self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) timestamp = bucket_info . creation_date return datetime . fromtimestamp ( timestamp ) def modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or self . isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) simplifiedmeta = self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( simplifiedmeta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () ) def append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket_name , obj_name = self . split_path ( path ) result = self . _call_oss ( \"append_object\" , obj_name , location , value , bucket = bucket_name , ) return result . next_position def get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket_name , obj_name = self . split_path ( path ) try : object_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name , byte_range = ( start , end ), headers = headers , ) except oss2 . exceptions . ServerError as err : raise err return object_stream . read () def sign ( self , path : str , expiration : int = 100 , ** kwargs ): raise NotImplementedError ( \"Sign is not implemented for this filesystem\" ) def pipe_file ( self , path : str , value : str , ** kwargs ): \"\"\"Set the bytes of given file\"\"\" bucket , key = self . split_path ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT self . invalidate_cache ( path ) if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return mpu : \"InitMultipartUploadResult\" = self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): data = value [ off : off + block_size ] part_number = i + 1 out : \"PutObjectResult\" = self . _call_oss ( \"upload_part\" , key , mpu . upload_id , part_number , data , bucket = bucket , ) parts . append ( PartInfo ( part_number , out . etag , size = len ( data ), part_crc = out . crc , ) ) self . _call_oss ( \"complete_multipart_upload\" , key , mpu . upload_id , parts , bucket = bucket , ) @prettify_info_result def info ( self , path , ** kwargs ): norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : result = { \"name\" : path , \"size\" : 0 , \"type\" : \"directory\" } else : result = super () . info ( path , ** kwargs ) if \"StorageClass\" in result : del result [ \"StorageClass\" ] if \"CreateTime\" in result : del result [ \"CreateTime\" ] return result def cat_file ( self , path : str , start : int = None , end : int = None , ** kwargs ): bucket , object_name = self . split_path ( path ) object_stream : \"GetObjectResult\" = self . _call_oss ( \"get_object\" , bucket = bucket , key = object_name , byte_range = ( start , end ), ** kwargs , ) return object_stream . read () append_object ( path , location , value ) Append bytes to the object Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 478 479 480 481 482 483 484 485 486 487 488 489 490 def append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket_name , obj_name = self . split_path ( path ) result = self . _call_oss ( \"append_object\" , obj_name , location , value , bucket = bucket_name , ) return result . next_position checksum ( path ) Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents might have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def checksum ( self , path : str ): \"\"\"Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) \"\"\" return sha256 ( ( str ( self . ukey ( path )) + str ( self . info ( path ))) . encode () ) . hexdigest () cp_file ( path1 , path2 , kwargs ) Copy within two locations in the filesystem todo: big file optimization Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\" Copy within two locations in the filesystem # todo: big file optimization \"\"\" bucket_name1 , obj_name1 = self . split_path ( path1 ) bucket_name2 , obj_name2 = self . split_path ( path2 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket_name1 != bucket_name2 : tempdir = \".\" + self . ukey ( path1 ) self . get_file ( path1 , tempdir , ** kwargs ) self . put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) self . _call_oss ( \"copy_object\" , bucket_name1 , obj_name1 , obj_name2 , bucket = bucket_name1 , timeout = connect_timeout , ) created ( path ) Return the created timestamp of a file as a datetime.datetime Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 456 457 458 459 460 461 462 463 def created ( self , path : str ): \"\"\"Return the created timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if obj_name : raise NotImplementedError ( \"OSS has no created timestamp\" ) bucket_info = self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) timestamp = bucket_info . creation_date return datetime . fromtimestamp ( timestamp ) exists ( path , kwargs ) Is there a file at the given path Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 def exists ( self , path : str , ** kwargs ) -> bool : \"\"\"Is there a file at the given path\"\"\" norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : return True bucket_name , obj_name = self . split_path ( path ) if not self . _bucket_exist ( bucket_name ): return False connect_timeout = kwargs . get ( \"connect_timeout\" , None ) if not obj_name : return True if self . _call_oss ( \"object_exists\" , obj_name , bucket = bucket_name , timeout = connect_timeout , ): return True return self . _directory_exists ( path , ** kwargs ) find ( path , maxdepth = None , withdirs = False , detail = False , kwargs ) List all files below path. Like posix find command without conditions Parameters path : str int or None If not None, the maximum number of levels to descend bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ls . Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @prettify_info_result def find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ``ls``. \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : connect_timeout = kwargs . get ( \"connect_timeout\" , None ) for info in self . _ls_dir ( path , delimiter = \"\" , prefix = prefix , connect_timeout = connect_timeout ): out . update ({ info [ \"name\" ]: info }) else : for _ , dirs , files in self . walk ( path , maxdepth , detail = True , ** kwargs ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for name , info in files . items ()}) if self . isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names } get_file ( rpath , lpath , callback = None , kwargs ) Copy single remote file to local Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def get_file ( self , rpath : str , lpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single remote file to local \"\"\" bucket_name , obj_name = self . split_path ( rpath ) kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) return connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if self . size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_download ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"get_object_to_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) get_object ( path , start , end ) Return object bytes in range Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket_name , obj_name = self . split_path ( path ) try : object_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name , byte_range = ( start , end ), headers = headers , ) except oss2 . exceptions . ServerError as err : raise err return object_stream . read () get_path ( rpath , lpath , kwargs ) Copy single remote path to local Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 393 394 395 396 397 398 399 400 def get_path ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote path to local \"\"\" if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) else : self . get_file ( rpath , lpath , ** kwargs ) modified ( path ) Return the modified timestamp of a file as a datetime.datetime Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 465 466 467 468 469 470 471 472 473 474 475 476 def modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or self . isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) simplifiedmeta = self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( simplifiedmeta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () ) pipe_file ( path , value , kwargs ) Set the bytes of given file Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 def pipe_file ( self , path : str , value : str , ** kwargs ): \"\"\"Set the bytes of given file\"\"\" bucket , key = self . split_path ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT self . invalidate_cache ( path ) if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return mpu : \"InitMultipartUploadResult\" = self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): data = value [ off : off + block_size ] part_number = i + 1 out : \"PutObjectResult\" = self . _call_oss ( \"upload_part\" , key , mpu . upload_id , part_number , data , bucket = bucket , ) parts . append ( PartInfo ( part_number , out . etag , size = len ( data ), part_crc = out . crc , ) ) self . _call_oss ( \"complete_multipart_upload\" , key , mpu . upload_id , parts , bucket = bucket , ) put_file ( lpath , rpath , callback = None , kwargs ) Copy single file to remote Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 def put_file ( self , lpath : str , rpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single file to remote \"\"\" kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) bucket_name , obj_name = self . split_path ( rpath ) if os . path . isdir ( lpath ): if obj_name : # don't make remote \"directory\" return self . mkdir ( lpath ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_upload ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"put_object_from_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) self . invalidate_cache ( self . _parent ( rpath )) rm ( path , recursive = False , maxdepth = None ) Delete files. Parameters str or list of str File(s) to delete. bool If file(s) are directories, recursively delete contents and then also remove the directory int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 def rm ( self , path : Union [ str , List [ str ]], recursive = False , maxdepth = None ): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. recursive: bool If file(s) are directories, recursively delete contents and then also remove the directory maxdepth: int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. \"\"\" if isinstance ( path , list ): for file in path : self . rm ( file ) return path_expand = self . expand_path ( path , recursive = recursive , maxdepth = maxdepth ) def chunks ( lst : list , num : int ): for i in range ( 0 , len ( lst ), num ): yield lst [ i : i + num ] for files in chunks ( path_expand , 1000 ): self . _bulk_delete ( files ) ukey ( path ) Hash of file properties, to tell if it has changed Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 287 288 289 290 291 def ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc","title":"Ossfs"},{"location":"reference/ossfs/#ossfs--ossfs","text":"A pythonic file-systems interface to OSS (Object Storage Service)","title":"OSSFS"},{"location":"reference/ossfs/#ossfs.AioOSSFileSystem","text":"Bases: BaseOSSFileSystem , AsyncFileSystem A pythonic file-systems interface to OSS (Object Storage Service) Base on async operations.","title":"AioOSSFileSystem"},{"location":"reference/ossfs/#ossfs.AioOSSFileSystem--examples","text":"ossfs = AioOSSFileSystem(anon=False) ossfs.ls('my-bucket/') ['my-file.txt'] with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 class AioOSSFileSystem ( BaseOSSFileSystem , AsyncFileSystem ): # pylint: disable=abstract-method \"\"\" A pythonic file-systems interface to OSS (Object Storage Service) Base on async operations. Examples -------- >>> ossfs = AioOSSFileSystem(anon=False) >>> ossfs.ls('my-bucket/') ['my-file.txt'] >>> with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' \"\"\" # pylint:disable=no-value-for-parameter protocol = \"oss\" def __init__ ( self , psize : int = DEFAULT_POOL_SIZE , ** kwargs , ): \"\"\" ---------------------------------------------------------------- Addition arguments Args: psize (int, optional): concurrency number of the connections to the server. Defaults to DEFAULT_POOL_SIZE. \"\"\" super () . __init__ ( ** kwargs ) self . _psize = psize self . _session : Optional [ \"AioSession\" ] = None __init__ . __doc__ = ( BaseOSSFileSystem . __init__ . __doc__ + __init__ . __doc__ # type: ignore ) def _get_bucket ( self , bucket_name : str , connect_timeout : Optional [ int ] = None ) -> AioBucket : \"\"\" get the new aio bucket instance \"\"\" if self . _endpoint is None : raise ValueError ( \"endpoint is required\" ) try : return AioBucket ( auth = self . _auth , endpoint = self . _endpoint , bucket_name = bucket_name , connect_timeout = connect_timeout , session = self . _session , app_name = \"ossfs\" , ) except ClientError as err : raise FileNotFoundError ( bucket_name ) from err async def set_session ( self , refresh : bool = False ): \"\"\"Establish a connection session object. Returns ------- Session to be closed later with await .close() \"\"\" logger . debug ( \"Connect AioSession instance\" ) if self . _session is None or self . _session . closed or refresh : if self . _session is None : self . _session = AioSession ( self . _psize ) await self . _session . __aenter__ () # pylint: disable=unnecessary-dunder-call # the following actually closes the aiohttp connection; use of privates # might break in the future, would cause exception at gc time if not self . asynchronous : weakref . finalize ( self , self . close_session ) return def close_session ( self ): \"\"\"Close a connection session object.\"\"\" if self . _session is None or self . _session . closed : return if self . loop is not None and self . loop . is_running (): try : sync ( self . loop , self . _session . close , timeout = 0.1 ) return except FSTimeoutError : pass async def _call_oss ( self , method_name : str , * args , bucket : Optional [ str ] = None , timeout : Optional [ int ] = None , ** kwargs , ): if self . _endpoint is None : raise ValueError ( \"endpoint is required\" ) await self . set_session () if bucket : service : Union [ AioService , AioBucket ] = self . _get_bucket ( bucket , timeout ) else : service = AioService ( auth = self . _auth , endpoint = self . _endpoint , session = self . _session , connect_timeout = timeout , app_name = \"ossfs\" , ) method = getattr ( service , method_name , None ) try : if not method : method = getattr ( aiooss2 , method_name ) logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( service , * args , ** kwargs ) else : logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = await method ( * args , ** kwargs ) return out except ( RequestError , OssError ) as err : error = err raise translate_oss_error ( error ) from error async def _ls_dir ( # pylint: disable=too-many-arguments self , path : str , refresh : bool = False , max_items : int = 100 , delimiter : str = \"/\" , prefix : str = \"\" , ): norm_path = path . strip ( \"/\" ) if norm_path in self . dircache and not refresh and not prefix and delimiter : return self . dircache [ norm_path ] logger . debug ( \"Get directory listing for %s \" , norm_path ) bucket , key = self . split_path ( norm_path ) prefix = prefix or \"\" if key : prefix = f \" { key } / { prefix } \" files = [] async for obj_dict in self . _iterdir ( bucket , max_keys = max_items , delimiter = delimiter , prefix = prefix , ): files . append ( obj_dict ) if not prefix and delimiter == \"/\" : self . dircache [ norm_path ] = files return files async def _iterdir ( self , bucket : str , max_keys : int = 100 , delimiter : str = \"/\" , prefix : str = \"\" , ): \"\"\"Iterate asynchronously over files and directories under `prefix`. The contents are yielded in arbitrary order as info dicts. \"\"\" response = await self . _call_oss ( \"AioObjectIterator\" , bucket = bucket , prefix = prefix , delimiter = delimiter , max_keys = max_keys , ) obj : \"SimplifiedObjectInfo\" async for obj in response : data = self . _transfer_object_info_to_dict ( bucket , obj ) yield data async def _ls_buckets ( self , refresh : bool = False ) -> List [ Dict [ str , Any ]]: if \"\" not in self . dircache or refresh : if isinstance ( self . _auth , AnonymousAuth ): logging . warning ( \"cannot list buckets if not logged in\" ) return [] results : List [ Dict [ str , Any ]] = [] try : files : \"ListBucketsResult\" = await self . _call_oss ( \"list_buckets\" ) except ClientError : # listbucket permission missing return [] file : \"SimplifiedBucketInfo\" for file in files . buckets : data : Dict [ str , Any ] = {} data [ \"name\" ] = file . name data [ \"size\" ] = 0 data [ \"type\" ] = \"directory\" results . append ( data ) self . dircache [ \"\" ] = results else : results = self . dircache [ \"\" ] return results @async_prettify_info_result async def _ls ( self , path : str , detail : bool = True , ** kwargs ): \"\"\"List files in given bucket, or list of buckets. Listing is cached unless `refresh=True`. Note: only your buckets associated with the login will be listed by `ls('')`, not any public buckets (even if already accessed). Parameters ---------- path : string/bytes location at which to list files refresh : bool (=False) if False, look in local cache for file details first \"\"\" refresh = kwargs . pop ( \"refresh\" , False ) norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) if norm_path != \"\" : files = await self . _ls_dir ( path , refresh ) if not files and \"/\" in norm_path : files = await self . _ls_dir ( self . _parent ( path ), refresh = refresh ) files = [ file for file in files if file [ \"name\" ] . strip ( \"/\" ) == norm_path and file [ \"type\" ] != \"directory\" ] else : files = await self . _ls_buckets ( refresh ) return files @async_prettify_info_result async def _info ( self , path : str , ** kwargs ): norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : result = { \"name\" : path , \"size\" : 0 , \"type\" : \"directory\" } return result bucket , key = self . split_path ( norm_path ) self . _get_bucket ( bucket ) refresh = kwargs . pop ( \"refresh\" , False ) if not refresh : out = self . _ls_from_cache ( norm_path ) if out is not None : out = [ o for o in out if o [ \"name\" ] . strip ( \"/\" ) == norm_path ] if out : result = out [ 0 ] else : result = { \"name\" : norm_path , \"size\" : 0 , \"type\" : \"directory\" } return result if key : try : obj_out : \"HeadObjectResult\" = await self . _call_oss ( \"head_object\" , key , bucket = bucket , ) result = { \"LastModified\" : obj_out . last_modified , \"size\" : obj_out . content_length , \"name\" : path , \"type\" : \"file\" , } return result except ( PermissionError , FileNotFoundError ): pass # We check to see if the path is a directory by attempting to list its # contexts. If anything is found, it is indeed a directory try : ls_out = await self . _call_oss ( \"AioObjectIterator\" , bucket = bucket , prefix = key . rstrip ( \"/\" ) + \"/\" , delimiter = \"/\" , max_keys = 100 , ) try : async for _ in ls_out : return { \"size\" : 0 , \"name\" : path , \"type\" : \"directory\" , } except OssError as err : raise translate_oss_error ( err ) from err except ( PermissionError , FileNotFoundError ): pass else : for bucket_info in await self . _ls_buckets (): if bucket_info [ \"name\" ] == norm_path . rstrip ( \"/\" ): return { \"size\" : 0 , \"name\" : path , \"type\" : \"directory\" , } raise FileNotFoundError ( path ) def _cache_result_analysis ( self , norm_path : str , parent : str ) -> bool : if norm_path in self . dircache : for file_info in self . dircache [ norm_path ]: # For files the dircache can contain itself. # If it contains anything other than itself it is a directory. if file_info [ \"name\" ] != norm_path : return True return False for file_info in self . dircache [ parent ]: if file_info [ \"name\" ] == norm_path : # If we find ourselves return whether we are a directory return file_info [ \"type\" ] == \"directory\" return False async def _isdir ( self , path : str ) -> bool : norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) # Send buckets to super if norm_path == \"\" : return True if \"/\" not in norm_path : for bucket_info in await self . _ls_buckets (): if bucket_info [ \"name\" ] == norm_path : return True return False parent = self . _parent ( norm_path ) if norm_path in self . dircache or parent in self . dircache : return self . _cache_result_analysis ( norm_path , parent ) # This only returns things within the path and NOT the path object itself try : return bool ( await self . _ls_dir ( norm_path )) except FileNotFoundError : return False async def _put_file ( self , lpath : str , rpath : str , ** kwargs ): bucket , key = self . split_path ( rpath ) if os . path . isdir ( lpath ): if key : # don't make remote \"directory\" return await self . _mkdir ( lpath ) else : callback = as_progress_handler ( kwargs . get ( \"callback\" , None )) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : await self . _call_oss ( \"resumable_upload\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) else : await self . _call_oss ( \"put_object_from_file\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) self . invalidate_cache ( self . _parent ( rpath )) async def _get_file ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote file to local \"\"\" bucket , key = self . split_path ( rpath ) if await self . _isdir ( rpath ): # don't make local \"directory\" return callback = as_progress_handler ( kwargs . get ( \"callback\" , None )) if await self . _size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : await self . _call_oss ( \"resumable_download\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) else : await self . _call_oss ( \"get_object_to_file\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ** kwargs , ) @async_prettify_info_result async def _find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , # pylint: disable=unused-argument ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. prefix: str Only return files that match ``^{path}/{prefix}`` (if there is an exact match ``filename == {path}/{prefix}``, it also will be included) \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : for info in await self . _ls_dir ( path , delimiter = \"\" , prefix = prefix ): out . update ({ info [ \"name\" ]: info }) else : async for _ , dirs , files in self . _walk ( path , maxdepth , detail = True ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for _ , info in files . items ()}) if await self . _isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names } async def _bulk_delete ( self , pathlist , ** kwargs ): \"\"\" Remove multiple keys with one call Parameters ---------- pathlist : list(str) The keys to remove, must all be in the same bucket. Must have 0 < len <= 1000 \"\"\" if not pathlist : return bucket , key_list = self . _get_batch_delete_key_list ( pathlist ) await self . _call_oss ( \"batch_delete_objects\" , key_list , bucket = bucket ) async def _rm_file ( self , path : str , ** kwargs ): bucket , key = self . split_path ( path ) await self . _call_oss ( \"delete_object\" , bucket = bucket , key = key ) self . invalidate_cache ( self . _parent ( path )) async def _rm ( self , path , recursive = False , batch_size = 1000 , ** kwargs ): if isinstance ( path , list ): for file in path : await self . _rm ( file ) return paths = await self . _expand_path ( path , recursive = recursive ) await _run_coros_in_chunks ( [ self . _bulk_delete ( paths [ i : i + batch_size ]) for i in range ( 0 , len ( paths ), batch_size ) ], batch_size = 3 , nofiles = True , ) async def _checksum ( self , path , refresh = True ): \"\"\" Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. Parameters ---------- path : string/bytes path of file to get checksum for refresh : bool (=False) if False, look in local cache for file details first \"\"\" return sha256 ( ( str ( await self . _ukey ( path )) + str ( await self . _info ( path , refresh = refresh )) ) . encode () ) . hexdigest () checksum = sync_wrapper ( _checksum ) async def _ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = await self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc checksum = sync_wrapper ( _checksum ) async def _cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\"Copy file between locations on OSS. preserve_etag: bool Whether to preserve etag while copying. If the file is uploaded as a single part, then it will be always equalivent to the md5 hash of the file hence etag will always be preserved. But if the file is uploaded in multi parts, then this option will try to reproduce the same multipart upload while copying and preserve the generated etag. \"\"\" bucket2 , key2 = self . split_path ( path2 ) bucket1 , key1 = self . split_path ( path1 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket1 != bucket2 : tempdir = \".\" + self . ukey ( path1 ) await self . _get_file ( path1 , tempdir , ** kwargs ) await self . _put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) await self . _call_oss ( \"copy_object\" , bucket1 , key1 , key2 , bucket = bucket1 , timeout = connect_timeout , ) async def _append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket , key = self . split_path ( path ) result : \"AppendObjectResult\" = await self . _call_oss ( \"append_object\" , key , location , value , bucket = bucket , ) return result . next_position append_object = sync_wrapper ( _append_object ) async def _get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket , key = self . split_path ( path ) object_stream : \"AioGetObjectResult\" = await self . _call_oss ( \"get_object\" , key , bucket = bucket , byte_range = ( start , end ), headers = headers , ) results = b \"\" while True : result = await object_stream . read () if result : results += result else : break return results get_object = sync_wrapper ( _get_object ) async def _pipe_file ( self , path : str , value : Union [ str , bytes ], ** kwargs ): bucket , key = self . split_path ( path ) self . invalidate_cache ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): await self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return init_multi_part_upload_result : \"InitMultipartUploadResult\" = ( await self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): part_number = i + 1 value_block = value [ off : off + block_size ] put_object_result : \"PutObjectResult\" = await self . _call_oss ( \"upload_part\" , key , init_multi_part_upload_result . upload_id , part_number , value_block , bucket = bucket , ) parts . append ( PartInfo ( part_number , put_object_result . etag , size = len ( value_block ), part_crc = put_object_result . crc , ) ) await self . _call_oss ( \"complete_multipart_upload\" , key , init_multi_part_upload_result . upload_id , parts , bucket = bucket , ) async def _cat_file ( self , path : str , start = None , end = None , ** kwargs ): bucket , key = self . split_path ( path ) object_stream : \"AioGetObjectResult\" = await self . _call_oss ( \"get_object\" , bucket = bucket , key = key , byte_range = ( start , end ), ** kwargs , ) results = b \"\" while True : result = await object_stream . read () if not result : break results += result return results async def _modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or await self . _isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) object_meta = await self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( object_meta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () ) modified = sync_wrapper ( _modified )","title":"Examples"},{"location":"reference/ossfs/#ossfs.async_oss.AioOSSFileSystem.__init__","text":"Addition arguments Parameters: Name Type Description Default psize int concurrency number of the connections to DEFAULT_POOL_SIZE Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , psize : int = DEFAULT_POOL_SIZE , ** kwargs , ): \"\"\" ---------------------------------------------------------------- Addition arguments Args: psize (int, optional): concurrency number of the connections to the server. Defaults to DEFAULT_POOL_SIZE. \"\"\" super () . __init__ ( ** kwargs ) self . _psize = psize self . _session : Optional [ \"AioSession\" ] = None","title":"__init__()"},{"location":"reference/ossfs/#ossfs.async_oss.AioOSSFileSystem.close_session","text":"Close a connection session object. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 122 123 124 125 126 127 128 129 130 131 def close_session ( self ): \"\"\"Close a connection session object.\"\"\" if self . _session is None or self . _session . closed : return if self . loop is not None and self . loop . is_running (): try : sync ( self . loop , self . _session . close , timeout = 0.1 ) return except FSTimeoutError : pass","title":"close_session()"},{"location":"reference/ossfs/#ossfs.async_oss.AioOSSFileSystem.set_session","text":"Establish a connection session object. Returns Session to be closed later with await .close() Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 async def set_session ( self , refresh : bool = False ): \"\"\"Establish a connection session object. Returns ------- Session to be closed later with await .close() \"\"\" logger . debug ( \"Connect AioSession instance\" ) if self . _session is None or self . _session . closed or refresh : if self . _session is None : self . _session = AioSession ( self . _psize ) await self . _session . __aenter__ () # pylint: disable=unnecessary-dunder-call # the following actually closes the aiohttp connection; use of privates # might break in the future, would cause exception at gc time if not self . asynchronous : weakref . finalize ( self , self . close_session ) return","title":"set_session()"},{"location":"reference/ossfs/#ossfs.OSSFile","text":"Bases: AbstractBufferedFile A file living in OSSFileSystem Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/file.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class OSSFile ( AbstractBufferedFile ): \"\"\"A file living in OSSFileSystem\"\"\" fs : Union [ \"OSSFileSystem\" , \"AioOSSFileSystem\" ] loc : int def _upload_chunk ( self , final : bool = False ) -> bool : \"\"\"Write one part of a multi-block file upload Parameters ========== final: bool This is the last block, so should complete file, if self.autocommit is True. \"\"\" self . loc = self . fs . append_object ( self . path , self . loc , self . buffer . getvalue ()) return True def _initiate_upload ( self ): \"\"\"Create remote file/upload\"\"\" if \"a\" in self . mode : self . loc = 0 if self . fs . exists ( self . path ): self . loc = self . fs . info ( self . path )[ \"size\" ] elif \"w\" in self . mode : # create empty file to append to self . loc = 0 if self . fs . exists ( self . path ): self . fs . rm_file ( self . path ) def _fetch_range ( self , start : int , end : int ) -> bytes : \"\"\" Get the specified set of bytes from remote Parameters ========== start: int end: int \"\"\" start = max ( start , 0 ) end = min ( self . size , end ) if start >= end or start >= self . size : return b \"\" return self . fs . get_object ( self . path , start , end )","title":"OSSFile"},{"location":"reference/ossfs/#ossfs.OSSFileSystem","text":"Bases: BaseOSSFileSystem A pythonic file-systems interface to OSS (Object Storage Service)","title":"OSSFileSystem"},{"location":"reference/ossfs/#ossfs.OSSFileSystem--examples","text":"ossfs = OSSFileSystem(anon=False) ossfs.ls('my-bucket/') ['my-file.txt'] with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 class OSSFileSystem ( BaseOSSFileSystem ): # pylint:disable=too-many-public-methods # pylint:disable=no-value-for-parameter \"\"\" A pythonic file-systems interface to OSS (Object Storage Service) Examples -------- >>> ossfs = OSSFileSystem(anon=False) >>> ossfs.ls('my-bucket/') ['my-file.txt'] >>> with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _session = oss2 . Session () def _get_bucket ( self , bucket_name : str , connect_timeout : Optional [ int ] = None ) -> oss2 . Bucket : \"\"\" get the new bucket instance \"\"\" if not self . _endpoint : raise ValueError ( \"endpoint is required\" ) try : return oss2 . Bucket ( self . _auth , self . _endpoint , bucket_name , session = self . _session , connect_timeout = connect_timeout , app_name = \"ossfs\" , ) except oss2 . exceptions . ClientError as err : raise FileNotFoundError ( bucket_name ) from err def _call_oss ( self , method_name : str , * args , bucket : Optional [ str ] = None , timeout : Optional [ int ] = None , retry : int = 3 , ** kwargs , ): if bucket : service = self . _get_bucket ( bucket , timeout ) else : service = oss2 . Service ( self . _auth , endpoint = self . _endpoint , connect_timeout = timeout , ) for count in range ( retry ): try : method = getattr ( service , method_name , None ) if not method : method = getattr ( oss2 , method_name ) logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( service , * args , ** kwargs ) else : logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( * args , ** kwargs ) return out except oss2 . exceptions . RequestError as err : logger . debug ( \"Retryable error: %s , try %s times\" , err , count + 1 ) error = err except oss2 . exceptions . OssError as err : logger . debug ( \"Nonretryable error: %s \" , err ) error = err break raise translate_oss_error ( error ) from error def _ls_bucket ( self , connect_timeout : Optional [ int ]) -> List [ Dict [ str , Any ]]: if \"\" not in self . dircache : results : List [ Dict [ str , Any ]] = [] if isinstance ( self . _auth , AnonymousAuth ): logging . warning ( \"cannot list buckets if not logged in\" ) return [] try : for bucket in self . _call_oss ( \"BucketIterator\" , timeout = connect_timeout ): result = { \"name\" : bucket . name , \"type\" : \"directory\" , \"size\" : 0 , \"CreateTime\" : bucket . creation_date , } results . append ( result ) except oss2 . exceptions . ClientError : pass self . dircache [ \"\" ] = copy . deepcopy ( results ) else : results = self . dircache [ \"\" ] return results def _get_object_info_list ( self , bucket_name : str , prefix : str , delimiter : str , connect_timeout : Optional [ int ], ): \"\"\" Wrap oss2.ObjectIterator return values into a fsspec form of file info \"\"\" result = [] obj : \"SimplifiedObjectInfo\" for obj in self . _call_oss ( \"ObjectIterator\" , prefix = prefix , delimiter = delimiter , bucket = bucket_name , timeout = connect_timeout , ): data = self . _transfer_object_info_to_dict ( bucket_name , obj ) result . append ( data ) return result def _ls_dir ( self , path : str , delimiter : str = \"/\" , refresh : bool = False , prefix : str = \"\" , connect_timeout : Optional [ int ] = None , ** kwargs , # pylint: disable=too-many-arguments ) -> List [ Dict ]: norm_path = path . strip ( \"/\" ) if norm_path in self . dircache and not refresh and not prefix and delimiter : return self . dircache [ norm_path ] logger . debug ( \"Get directory listing page for %s \" , norm_path ) bucket_name , key = self . split_path ( norm_path ) if not delimiter or prefix : if key : prefix = f \" { key } / { prefix } \" else : if norm_path in self . dircache and not refresh : return self . dircache [ norm_path ] if key : prefix = f \" { key } /\" try : self . dircache [ norm_path ] = self . _get_object_info_list ( bucket_name , prefix , delimiter , connect_timeout ) return self . dircache [ norm_path ] except oss2 . exceptions . AccessDenied : return [] @prettify_info_result def ls ( self , path : str , detail : bool = True , ** kwargs ): connect_timeout = kwargs . pop ( \"connect_timeout\" , 60 ) norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) if norm_path == \"\" : return self . _ls_bucket ( connect_timeout ) files = self . _ls_dir ( path , connect_timeout = connect_timeout ) if not files and \"/\" in norm_path : files = self . _ls_dir ( self . _parent ( path ), connect_timeout = connect_timeout ) files = [ file for file in files if file [ \"type\" ] != \"directory\" and file [ \"name\" ] . strip ( \"/\" ) == norm_path ] return files @prettify_info_result def find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ``ls``. \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : connect_timeout = kwargs . get ( \"connect_timeout\" , None ) for info in self . _ls_dir ( path , delimiter = \"\" , prefix = prefix , connect_timeout = connect_timeout ): out . update ({ info [ \"name\" ]: info }) else : for _ , dirs , files in self . walk ( path , maxdepth , detail = True , ** kwargs ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for name , info in files . items ()}) if self . isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names } def _directory_exists ( self , dirname : str , ** kwargs ): connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) ls_result = self . _ls_dir ( dirname , connect_timeout = connect_timeout ) return bool ( ls_result ) def _bucket_exist ( self , bucket_name : str ): if not bucket_name : return False try : self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) except ( oss2 . exceptions . OssError , PermissionError ): return False return True def exists ( self , path : str , ** kwargs ) -> bool : \"\"\"Is there a file at the given path\"\"\" norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : return True bucket_name , obj_name = self . split_path ( path ) if not self . _bucket_exist ( bucket_name ): return False connect_timeout = kwargs . get ( \"connect_timeout\" , None ) if not obj_name : return True if self . _call_oss ( \"object_exists\" , obj_name , bucket = bucket_name , timeout = connect_timeout , ): return True return self . _directory_exists ( path , ** kwargs ) def ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc def checksum ( self , path : str ): \"\"\"Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) \"\"\" return sha256 ( ( str ( self . ukey ( path )) + str ( self . info ( path ))) . encode () ) . hexdigest () def cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\" Copy within two locations in the filesystem # todo: big file optimization \"\"\" bucket_name1 , obj_name1 = self . split_path ( path1 ) bucket_name2 , obj_name2 = self . split_path ( path2 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket_name1 != bucket_name2 : tempdir = \".\" + self . ukey ( path1 ) self . get_file ( path1 , tempdir , ** kwargs ) self . put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) self . _call_oss ( \"copy_object\" , bucket_name1 , obj_name1 , obj_name2 , bucket = bucket_name1 , timeout = connect_timeout , ) def _rm ( self , path : Union [ str , List [ str ]]): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. \"\"\" if isinstance ( path , list ): for file in path : self . _rm ( file ) return bucket_name , obj_name = self . split_path ( path ) self . invalidate_cache ( self . _parent ( path )) self . _call_oss ( \"delete_object\" , obj_name , bucket = bucket_name ) def _bulk_delete ( self , pathlist , ** kwargs ): \"\"\" Remove multiple keys with one call Parameters ---------- pathlist : list(str) The keys to remove, must all be in the same bucket. Must have 0 < len <= 1000 \"\"\" if not pathlist : return bucket , key_list = self . _get_batch_delete_key_list ( pathlist ) self . _call_oss ( \"batch_delete_objects\" , key_list , bucket = bucket ) def rm ( self , path : Union [ str , List [ str ]], recursive = False , maxdepth = None ): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. recursive: bool If file(s) are directories, recursively delete contents and then also remove the directory maxdepth: int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. \"\"\" if isinstance ( path , list ): for file in path : self . rm ( file ) return path_expand = self . expand_path ( path , recursive = recursive , maxdepth = maxdepth ) def chunks ( lst : list , num : int ): for i in range ( 0 , len ( lst ), num ): yield lst [ i : i + num ] for files in chunks ( path_expand , 1000 ): self . _bulk_delete ( files ) def get_path ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote path to local \"\"\" if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) else : self . get_file ( rpath , lpath , ** kwargs ) def get_file ( self , rpath : str , lpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single remote file to local \"\"\" bucket_name , obj_name = self . split_path ( rpath ) kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) return connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if self . size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_download ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"get_object_to_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) def put_file ( self , lpath : str , rpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single file to remote \"\"\" kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) bucket_name , obj_name = self . split_path ( rpath ) if os . path . isdir ( lpath ): if obj_name : # don't make remote \"directory\" return self . mkdir ( lpath ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_upload ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"put_object_from_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) self . invalidate_cache ( self . _parent ( rpath )) def created ( self , path : str ): \"\"\"Return the created timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if obj_name : raise NotImplementedError ( \"OSS has no created timestamp\" ) bucket_info = self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) timestamp = bucket_info . creation_date return datetime . fromtimestamp ( timestamp ) def modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or self . isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) simplifiedmeta = self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( simplifiedmeta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () ) def append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket_name , obj_name = self . split_path ( path ) result = self . _call_oss ( \"append_object\" , obj_name , location , value , bucket = bucket_name , ) return result . next_position def get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket_name , obj_name = self . split_path ( path ) try : object_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name , byte_range = ( start , end ), headers = headers , ) except oss2 . exceptions . ServerError as err : raise err return object_stream . read () def sign ( self , path : str , expiration : int = 100 , ** kwargs ): raise NotImplementedError ( \"Sign is not implemented for this filesystem\" ) def pipe_file ( self , path : str , value : str , ** kwargs ): \"\"\"Set the bytes of given file\"\"\" bucket , key = self . split_path ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT self . invalidate_cache ( path ) if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return mpu : \"InitMultipartUploadResult\" = self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): data = value [ off : off + block_size ] part_number = i + 1 out : \"PutObjectResult\" = self . _call_oss ( \"upload_part\" , key , mpu . upload_id , part_number , data , bucket = bucket , ) parts . append ( PartInfo ( part_number , out . etag , size = len ( data ), part_crc = out . crc , ) ) self . _call_oss ( \"complete_multipart_upload\" , key , mpu . upload_id , parts , bucket = bucket , ) @prettify_info_result def info ( self , path , ** kwargs ): norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : result = { \"name\" : path , \"size\" : 0 , \"type\" : \"directory\" } else : result = super () . info ( path , ** kwargs ) if \"StorageClass\" in result : del result [ \"StorageClass\" ] if \"CreateTime\" in result : del result [ \"CreateTime\" ] return result def cat_file ( self , path : str , start : int = None , end : int = None , ** kwargs ): bucket , object_name = self . split_path ( path ) object_stream : \"GetObjectResult\" = self . _call_oss ( \"get_object\" , bucket = bucket , key = object_name , byte_range = ( start , end ), ** kwargs , ) return object_stream . read ()","title":"Examples"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.append_object","text":"Append bytes to the object Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 478 479 480 481 482 483 484 485 486 487 488 489 490 def append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket_name , obj_name = self . split_path ( path ) result = self . _call_oss ( \"append_object\" , obj_name , location , value , bucket = bucket_name , ) return result . next_position","title":"append_object()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.checksum","text":"Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents might have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def checksum ( self , path : str ): \"\"\"Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) \"\"\" return sha256 ( ( str ( self . ukey ( path )) + str ( self . info ( path ))) . encode () ) . hexdigest ()","title":"checksum()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.cp_file","text":"Copy within two locations in the filesystem","title":"cp_file()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.cp_file--todo-big-file-optimization","text":"Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\" Copy within two locations in the filesystem # todo: big file optimization \"\"\" bucket_name1 , obj_name1 = self . split_path ( path1 ) bucket_name2 , obj_name2 = self . split_path ( path2 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket_name1 != bucket_name2 : tempdir = \".\" + self . ukey ( path1 ) self . get_file ( path1 , tempdir , ** kwargs ) self . put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) self . _call_oss ( \"copy_object\" , bucket_name1 , obj_name1 , obj_name2 , bucket = bucket_name1 , timeout = connect_timeout , )","title":"todo: big file optimization"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.created","text":"Return the created timestamp of a file as a datetime.datetime Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 456 457 458 459 460 461 462 463 def created ( self , path : str ): \"\"\"Return the created timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if obj_name : raise NotImplementedError ( \"OSS has no created timestamp\" ) bucket_info = self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) timestamp = bucket_info . creation_date return datetime . fromtimestamp ( timestamp )","title":"created()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.exists","text":"Is there a file at the given path Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 def exists ( self , path : str , ** kwargs ) -> bool : \"\"\"Is there a file at the given path\"\"\" norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : return True bucket_name , obj_name = self . split_path ( path ) if not self . _bucket_exist ( bucket_name ): return False connect_timeout = kwargs . get ( \"connect_timeout\" , None ) if not obj_name : return True if self . _call_oss ( \"object_exists\" , obj_name , bucket = bucket_name , timeout = connect_timeout , ): return True return self . _directory_exists ( path , ** kwargs )","title":"exists()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.find","text":"List all files below path. Like posix find command without conditions","title":"find()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.find--parameters","text":"path : str int or None If not None, the maximum number of levels to descend bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ls . Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @prettify_info_result def find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ``ls``. \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : connect_timeout = kwargs . get ( \"connect_timeout\" , None ) for info in self . _ls_dir ( path , delimiter = \"\" , prefix = prefix , connect_timeout = connect_timeout ): out . update ({ info [ \"name\" ]: info }) else : for _ , dirs , files in self . walk ( path , maxdepth , detail = True , ** kwargs ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for name , info in files . items ()}) if self . isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names }","title":"Parameters"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.get_file","text":"Copy single remote file to local Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def get_file ( self , rpath : str , lpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single remote file to local \"\"\" bucket_name , obj_name = self . split_path ( rpath ) kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) return connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if self . size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_download ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"get_object_to_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , )","title":"get_file()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.get_object","text":"Return object bytes in range Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket_name , obj_name = self . split_path ( path ) try : object_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name , byte_range = ( start , end ), headers = headers , ) except oss2 . exceptions . ServerError as err : raise err return object_stream . read ()","title":"get_object()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.get_path","text":"Copy single remote path to local Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 393 394 395 396 397 398 399 400 def get_path ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote path to local \"\"\" if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) else : self . get_file ( rpath , lpath , ** kwargs )","title":"get_path()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.modified","text":"Return the modified timestamp of a file as a datetime.datetime Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 465 466 467 468 469 470 471 472 473 474 475 476 def modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or self . isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) simplifiedmeta = self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( simplifiedmeta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () )","title":"modified()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.pipe_file","text":"Set the bytes of given file Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 def pipe_file ( self , path : str , value : str , ** kwargs ): \"\"\"Set the bytes of given file\"\"\" bucket , key = self . split_path ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT self . invalidate_cache ( path ) if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return mpu : \"InitMultipartUploadResult\" = self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): data = value [ off : off + block_size ] part_number = i + 1 out : \"PutObjectResult\" = self . _call_oss ( \"upload_part\" , key , mpu . upload_id , part_number , data , bucket = bucket , ) parts . append ( PartInfo ( part_number , out . etag , size = len ( data ), part_crc = out . crc , ) ) self . _call_oss ( \"complete_multipart_upload\" , key , mpu . upload_id , parts , bucket = bucket , )","title":"pipe_file()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.put_file","text":"Copy single file to remote Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 def put_file ( self , lpath : str , rpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single file to remote \"\"\" kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) bucket_name , obj_name = self . split_path ( rpath ) if os . path . isdir ( lpath ): if obj_name : # don't make remote \"directory\" return self . mkdir ( lpath ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_upload ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"put_object_from_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) self . invalidate_cache ( self . _parent ( rpath ))","title":"put_file()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.rm","text":"Delete files.","title":"rm()"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.rm--parameters","text":"str or list of str File(s) to delete. bool If file(s) are directories, recursively delete contents and then also remove the directory int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 def rm ( self , path : Union [ str , List [ str ]], recursive = False , maxdepth = None ): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. recursive: bool If file(s) are directories, recursively delete contents and then also remove the directory maxdepth: int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. \"\"\" if isinstance ( path , list ): for file in path : self . rm ( file ) return path_expand = self . expand_path ( path , recursive = recursive , maxdepth = maxdepth ) def chunks ( lst : list , num : int ): for i in range ( 0 , len ( lst ), num ): yield lst [ i : i + num ] for files in chunks ( path_expand , 1000 ): self . _bulk_delete ( files )","title":"Parameters"},{"location":"reference/ossfs/#ossfs.core.OSSFileSystem.ukey","text":"Hash of file properties, to tell if it has changed Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 287 288 289 290 291 def ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc","title":"ukey()"},{"location":"reference/ossfs/async_oss/","text":"Code of AioOSSFileSystem AioOSSFileSystem Bases: BaseOSSFileSystem , AsyncFileSystem A pythonic file-systems interface to OSS (Object Storage Service) Base on async operations. Examples ossfs = AioOSSFileSystem(anon=False) ossfs.ls('my-bucket/') ['my-file.txt'] with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 class AioOSSFileSystem ( BaseOSSFileSystem , AsyncFileSystem ): # pylint: disable=abstract-method \"\"\" A pythonic file-systems interface to OSS (Object Storage Service) Base on async operations. Examples -------- >>> ossfs = AioOSSFileSystem(anon=False) >>> ossfs.ls('my-bucket/') ['my-file.txt'] >>> with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' \"\"\" # pylint:disable=no-value-for-parameter protocol = \"oss\" def __init__ ( self , psize : int = DEFAULT_POOL_SIZE , ** kwargs , ): \"\"\" ---------------------------------------------------------------- Addition arguments Args: psize (int, optional): concurrency number of the connections to the server. Defaults to DEFAULT_POOL_SIZE. \"\"\" super () . __init__ ( ** kwargs ) self . _psize = psize self . _session : Optional [ \"AioSession\" ] = None __init__ . __doc__ = ( BaseOSSFileSystem . __init__ . __doc__ + __init__ . __doc__ # type: ignore ) def _get_bucket ( self , bucket_name : str , connect_timeout : Optional [ int ] = None ) -> AioBucket : \"\"\" get the new aio bucket instance \"\"\" if self . _endpoint is None : raise ValueError ( \"endpoint is required\" ) try : return AioBucket ( auth = self . _auth , endpoint = self . _endpoint , bucket_name = bucket_name , connect_timeout = connect_timeout , session = self . _session , app_name = \"ossfs\" , ) except ClientError as err : raise FileNotFoundError ( bucket_name ) from err async def set_session ( self , refresh : bool = False ): \"\"\"Establish a connection session object. Returns ------- Session to be closed later with await .close() \"\"\" logger . debug ( \"Connect AioSession instance\" ) if self . _session is None or self . _session . closed or refresh : if self . _session is None : self . _session = AioSession ( self . _psize ) await self . _session . __aenter__ () # pylint: disable=unnecessary-dunder-call # the following actually closes the aiohttp connection; use of privates # might break in the future, would cause exception at gc time if not self . asynchronous : weakref . finalize ( self , self . close_session ) return def close_session ( self ): \"\"\"Close a connection session object.\"\"\" if self . _session is None or self . _session . closed : return if self . loop is not None and self . loop . is_running (): try : sync ( self . loop , self . _session . close , timeout = 0.1 ) return except FSTimeoutError : pass async def _call_oss ( self , method_name : str , * args , bucket : Optional [ str ] = None , timeout : Optional [ int ] = None , ** kwargs , ): if self . _endpoint is None : raise ValueError ( \"endpoint is required\" ) await self . set_session () if bucket : service : Union [ AioService , AioBucket ] = self . _get_bucket ( bucket , timeout ) else : service = AioService ( auth = self . _auth , endpoint = self . _endpoint , session = self . _session , connect_timeout = timeout , app_name = \"ossfs\" , ) method = getattr ( service , method_name , None ) try : if not method : method = getattr ( aiooss2 , method_name ) logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( service , * args , ** kwargs ) else : logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = await method ( * args , ** kwargs ) return out except ( RequestError , OssError ) as err : error = err raise translate_oss_error ( error ) from error async def _ls_dir ( # pylint: disable=too-many-arguments self , path : str , refresh : bool = False , max_items : int = 100 , delimiter : str = \"/\" , prefix : str = \"\" , ): norm_path = path . strip ( \"/\" ) if norm_path in self . dircache and not refresh and not prefix and delimiter : return self . dircache [ norm_path ] logger . debug ( \"Get directory listing for %s \" , norm_path ) bucket , key = self . split_path ( norm_path ) prefix = prefix or \"\" if key : prefix = f \" { key } / { prefix } \" files = [] async for obj_dict in self . _iterdir ( bucket , max_keys = max_items , delimiter = delimiter , prefix = prefix , ): files . append ( obj_dict ) if not prefix and delimiter == \"/\" : self . dircache [ norm_path ] = files return files async def _iterdir ( self , bucket : str , max_keys : int = 100 , delimiter : str = \"/\" , prefix : str = \"\" , ): \"\"\"Iterate asynchronously over files and directories under `prefix`. The contents are yielded in arbitrary order as info dicts. \"\"\" response = await self . _call_oss ( \"AioObjectIterator\" , bucket = bucket , prefix = prefix , delimiter = delimiter , max_keys = max_keys , ) obj : \"SimplifiedObjectInfo\" async for obj in response : data = self . _transfer_object_info_to_dict ( bucket , obj ) yield data async def _ls_buckets ( self , refresh : bool = False ) -> List [ Dict [ str , Any ]]: if \"\" not in self . dircache or refresh : if isinstance ( self . _auth , AnonymousAuth ): logging . warning ( \"cannot list buckets if not logged in\" ) return [] results : List [ Dict [ str , Any ]] = [] try : files : \"ListBucketsResult\" = await self . _call_oss ( \"list_buckets\" ) except ClientError : # listbucket permission missing return [] file : \"SimplifiedBucketInfo\" for file in files . buckets : data : Dict [ str , Any ] = {} data [ \"name\" ] = file . name data [ \"size\" ] = 0 data [ \"type\" ] = \"directory\" results . append ( data ) self . dircache [ \"\" ] = results else : results = self . dircache [ \"\" ] return results @async_prettify_info_result async def _ls ( self , path : str , detail : bool = True , ** kwargs ): \"\"\"List files in given bucket, or list of buckets. Listing is cached unless `refresh=True`. Note: only your buckets associated with the login will be listed by `ls('')`, not any public buckets (even if already accessed). Parameters ---------- path : string/bytes location at which to list files refresh : bool (=False) if False, look in local cache for file details first \"\"\" refresh = kwargs . pop ( \"refresh\" , False ) norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) if norm_path != \"\" : files = await self . _ls_dir ( path , refresh ) if not files and \"/\" in norm_path : files = await self . _ls_dir ( self . _parent ( path ), refresh = refresh ) files = [ file for file in files if file [ \"name\" ] . strip ( \"/\" ) == norm_path and file [ \"type\" ] != \"directory\" ] else : files = await self . _ls_buckets ( refresh ) return files @async_prettify_info_result async def _info ( self , path : str , ** kwargs ): norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : result = { \"name\" : path , \"size\" : 0 , \"type\" : \"directory\" } return result bucket , key = self . split_path ( norm_path ) self . _get_bucket ( bucket ) refresh = kwargs . pop ( \"refresh\" , False ) if not refresh : out = self . _ls_from_cache ( norm_path ) if out is not None : out = [ o for o in out if o [ \"name\" ] . strip ( \"/\" ) == norm_path ] if out : result = out [ 0 ] else : result = { \"name\" : norm_path , \"size\" : 0 , \"type\" : \"directory\" } return result if key : try : obj_out : \"HeadObjectResult\" = await self . _call_oss ( \"head_object\" , key , bucket = bucket , ) result = { \"LastModified\" : obj_out . last_modified , \"size\" : obj_out . content_length , \"name\" : path , \"type\" : \"file\" , } return result except ( PermissionError , FileNotFoundError ): pass # We check to see if the path is a directory by attempting to list its # contexts. If anything is found, it is indeed a directory try : ls_out = await self . _call_oss ( \"AioObjectIterator\" , bucket = bucket , prefix = key . rstrip ( \"/\" ) + \"/\" , delimiter = \"/\" , max_keys = 100 , ) try : async for _ in ls_out : return { \"size\" : 0 , \"name\" : path , \"type\" : \"directory\" , } except OssError as err : raise translate_oss_error ( err ) from err except ( PermissionError , FileNotFoundError ): pass else : for bucket_info in await self . _ls_buckets (): if bucket_info [ \"name\" ] == norm_path . rstrip ( \"/\" ): return { \"size\" : 0 , \"name\" : path , \"type\" : \"directory\" , } raise FileNotFoundError ( path ) def _cache_result_analysis ( self , norm_path : str , parent : str ) -> bool : if norm_path in self . dircache : for file_info in self . dircache [ norm_path ]: # For files the dircache can contain itself. # If it contains anything other than itself it is a directory. if file_info [ \"name\" ] != norm_path : return True return False for file_info in self . dircache [ parent ]: if file_info [ \"name\" ] == norm_path : # If we find ourselves return whether we are a directory return file_info [ \"type\" ] == \"directory\" return False async def _isdir ( self , path : str ) -> bool : norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) # Send buckets to super if norm_path == \"\" : return True if \"/\" not in norm_path : for bucket_info in await self . _ls_buckets (): if bucket_info [ \"name\" ] == norm_path : return True return False parent = self . _parent ( norm_path ) if norm_path in self . dircache or parent in self . dircache : return self . _cache_result_analysis ( norm_path , parent ) # This only returns things within the path and NOT the path object itself try : return bool ( await self . _ls_dir ( norm_path )) except FileNotFoundError : return False async def _put_file ( self , lpath : str , rpath : str , ** kwargs ): bucket , key = self . split_path ( rpath ) if os . path . isdir ( lpath ): if key : # don't make remote \"directory\" return await self . _mkdir ( lpath ) else : callback = as_progress_handler ( kwargs . get ( \"callback\" , None )) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : await self . _call_oss ( \"resumable_upload\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) else : await self . _call_oss ( \"put_object_from_file\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) self . invalidate_cache ( self . _parent ( rpath )) async def _get_file ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote file to local \"\"\" bucket , key = self . split_path ( rpath ) if await self . _isdir ( rpath ): # don't make local \"directory\" return callback = as_progress_handler ( kwargs . get ( \"callback\" , None )) if await self . _size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : await self . _call_oss ( \"resumable_download\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) else : await self . _call_oss ( \"get_object_to_file\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ** kwargs , ) @async_prettify_info_result async def _find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , # pylint: disable=unused-argument ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. prefix: str Only return files that match ``^{path}/{prefix}`` (if there is an exact match ``filename == {path}/{prefix}``, it also will be included) \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : for info in await self . _ls_dir ( path , delimiter = \"\" , prefix = prefix ): out . update ({ info [ \"name\" ]: info }) else : async for _ , dirs , files in self . _walk ( path , maxdepth , detail = True ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for _ , info in files . items ()}) if await self . _isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names } async def _bulk_delete ( self , pathlist , ** kwargs ): \"\"\" Remove multiple keys with one call Parameters ---------- pathlist : list(str) The keys to remove, must all be in the same bucket. Must have 0 < len <= 1000 \"\"\" if not pathlist : return bucket , key_list = self . _get_batch_delete_key_list ( pathlist ) await self . _call_oss ( \"batch_delete_objects\" , key_list , bucket = bucket ) async def _rm_file ( self , path : str , ** kwargs ): bucket , key = self . split_path ( path ) await self . _call_oss ( \"delete_object\" , bucket = bucket , key = key ) self . invalidate_cache ( self . _parent ( path )) async def _rm ( self , path , recursive = False , batch_size = 1000 , ** kwargs ): if isinstance ( path , list ): for file in path : await self . _rm ( file ) return paths = await self . _expand_path ( path , recursive = recursive ) await _run_coros_in_chunks ( [ self . _bulk_delete ( paths [ i : i + batch_size ]) for i in range ( 0 , len ( paths ), batch_size ) ], batch_size = 3 , nofiles = True , ) async def _checksum ( self , path , refresh = True ): \"\"\" Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. Parameters ---------- path : string/bytes path of file to get checksum for refresh : bool (=False) if False, look in local cache for file details first \"\"\" return sha256 ( ( str ( await self . _ukey ( path )) + str ( await self . _info ( path , refresh = refresh )) ) . encode () ) . hexdigest () checksum = sync_wrapper ( _checksum ) async def _ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = await self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc checksum = sync_wrapper ( _checksum ) async def _cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\"Copy file between locations on OSS. preserve_etag: bool Whether to preserve etag while copying. If the file is uploaded as a single part, then it will be always equalivent to the md5 hash of the file hence etag will always be preserved. But if the file is uploaded in multi parts, then this option will try to reproduce the same multipart upload while copying and preserve the generated etag. \"\"\" bucket2 , key2 = self . split_path ( path2 ) bucket1 , key1 = self . split_path ( path1 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket1 != bucket2 : tempdir = \".\" + self . ukey ( path1 ) await self . _get_file ( path1 , tempdir , ** kwargs ) await self . _put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) await self . _call_oss ( \"copy_object\" , bucket1 , key1 , key2 , bucket = bucket1 , timeout = connect_timeout , ) async def _append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket , key = self . split_path ( path ) result : \"AppendObjectResult\" = await self . _call_oss ( \"append_object\" , key , location , value , bucket = bucket , ) return result . next_position append_object = sync_wrapper ( _append_object ) async def _get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket , key = self . split_path ( path ) object_stream : \"AioGetObjectResult\" = await self . _call_oss ( \"get_object\" , key , bucket = bucket , byte_range = ( start , end ), headers = headers , ) results = b \"\" while True : result = await object_stream . read () if result : results += result else : break return results get_object = sync_wrapper ( _get_object ) async def _pipe_file ( self , path : str , value : Union [ str , bytes ], ** kwargs ): bucket , key = self . split_path ( path ) self . invalidate_cache ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): await self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return init_multi_part_upload_result : \"InitMultipartUploadResult\" = ( await self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): part_number = i + 1 value_block = value [ off : off + block_size ] put_object_result : \"PutObjectResult\" = await self . _call_oss ( \"upload_part\" , key , init_multi_part_upload_result . upload_id , part_number , value_block , bucket = bucket , ) parts . append ( PartInfo ( part_number , put_object_result . etag , size = len ( value_block ), part_crc = put_object_result . crc , ) ) await self . _call_oss ( \"complete_multipart_upload\" , key , init_multi_part_upload_result . upload_id , parts , bucket = bucket , ) async def _cat_file ( self , path : str , start = None , end = None , ** kwargs ): bucket , key = self . split_path ( path ) object_stream : \"AioGetObjectResult\" = await self . _call_oss ( \"get_object\" , bucket = bucket , key = key , byte_range = ( start , end ), ** kwargs , ) results = b \"\" while True : result = await object_stream . read () if not result : break results += result return results async def _modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or await self . _isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) object_meta = await self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( object_meta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () ) modified = sync_wrapper ( _modified ) __init__ ( psize = DEFAULT_POOL_SIZE , kwargs ) Addition arguments Parameters: Name Type Description Default psize int concurrency number of the connections to DEFAULT_POOL_SIZE Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , psize : int = DEFAULT_POOL_SIZE , ** kwargs , ): \"\"\" ---------------------------------------------------------------- Addition arguments Args: psize (int, optional): concurrency number of the connections to the server. Defaults to DEFAULT_POOL_SIZE. \"\"\" super () . __init__ ( ** kwargs ) self . _psize = psize self . _session : Optional [ \"AioSession\" ] = None close_session () Close a connection session object. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 122 123 124 125 126 127 128 129 130 131 def close_session ( self ): \"\"\"Close a connection session object.\"\"\" if self . _session is None or self . _session . closed : return if self . loop is not None and self . loop . is_running (): try : sync ( self . loop , self . _session . close , timeout = 0.1 ) return except FSTimeoutError : pass set_session ( refresh = False ) async Establish a connection session object. Returns Session to be closed later with await .close() Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 async def set_session ( self , refresh : bool = False ): \"\"\"Establish a connection session object. Returns ------- Session to be closed later with await .close() \"\"\" logger . debug ( \"Connect AioSession instance\" ) if self . _session is None or self . _session . closed or refresh : if self . _session is None : self . _session = AioSession ( self . _psize ) await self . _session . __aenter__ () # pylint: disable=unnecessary-dunder-call # the following actually closes the aiohttp connection; use of privates # might break in the future, would cause exception at gc time if not self . asynchronous : weakref . finalize ( self , self . close_session ) return","title":"Async oss"},{"location":"reference/ossfs/async_oss/#ossfs.async_oss.AioOSSFileSystem","text":"Bases: BaseOSSFileSystem , AsyncFileSystem A pythonic file-systems interface to OSS (Object Storage Service) Base on async operations.","title":"AioOSSFileSystem"},{"location":"reference/ossfs/async_oss/#ossfs.async_oss.AioOSSFileSystem--examples","text":"ossfs = AioOSSFileSystem(anon=False) ossfs.ls('my-bucket/') ['my-file.txt'] with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 class AioOSSFileSystem ( BaseOSSFileSystem , AsyncFileSystem ): # pylint: disable=abstract-method \"\"\" A pythonic file-systems interface to OSS (Object Storage Service) Base on async operations. Examples -------- >>> ossfs = AioOSSFileSystem(anon=False) >>> ossfs.ls('my-bucket/') ['my-file.txt'] >>> with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' \"\"\" # pylint:disable=no-value-for-parameter protocol = \"oss\" def __init__ ( self , psize : int = DEFAULT_POOL_SIZE , ** kwargs , ): \"\"\" ---------------------------------------------------------------- Addition arguments Args: psize (int, optional): concurrency number of the connections to the server. Defaults to DEFAULT_POOL_SIZE. \"\"\" super () . __init__ ( ** kwargs ) self . _psize = psize self . _session : Optional [ \"AioSession\" ] = None __init__ . __doc__ = ( BaseOSSFileSystem . __init__ . __doc__ + __init__ . __doc__ # type: ignore ) def _get_bucket ( self , bucket_name : str , connect_timeout : Optional [ int ] = None ) -> AioBucket : \"\"\" get the new aio bucket instance \"\"\" if self . _endpoint is None : raise ValueError ( \"endpoint is required\" ) try : return AioBucket ( auth = self . _auth , endpoint = self . _endpoint , bucket_name = bucket_name , connect_timeout = connect_timeout , session = self . _session , app_name = \"ossfs\" , ) except ClientError as err : raise FileNotFoundError ( bucket_name ) from err async def set_session ( self , refresh : bool = False ): \"\"\"Establish a connection session object. Returns ------- Session to be closed later with await .close() \"\"\" logger . debug ( \"Connect AioSession instance\" ) if self . _session is None or self . _session . closed or refresh : if self . _session is None : self . _session = AioSession ( self . _psize ) await self . _session . __aenter__ () # pylint: disable=unnecessary-dunder-call # the following actually closes the aiohttp connection; use of privates # might break in the future, would cause exception at gc time if not self . asynchronous : weakref . finalize ( self , self . close_session ) return def close_session ( self ): \"\"\"Close a connection session object.\"\"\" if self . _session is None or self . _session . closed : return if self . loop is not None and self . loop . is_running (): try : sync ( self . loop , self . _session . close , timeout = 0.1 ) return except FSTimeoutError : pass async def _call_oss ( self , method_name : str , * args , bucket : Optional [ str ] = None , timeout : Optional [ int ] = None , ** kwargs , ): if self . _endpoint is None : raise ValueError ( \"endpoint is required\" ) await self . set_session () if bucket : service : Union [ AioService , AioBucket ] = self . _get_bucket ( bucket , timeout ) else : service = AioService ( auth = self . _auth , endpoint = self . _endpoint , session = self . _session , connect_timeout = timeout , app_name = \"ossfs\" , ) method = getattr ( service , method_name , None ) try : if not method : method = getattr ( aiooss2 , method_name ) logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( service , * args , ** kwargs ) else : logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = await method ( * args , ** kwargs ) return out except ( RequestError , OssError ) as err : error = err raise translate_oss_error ( error ) from error async def _ls_dir ( # pylint: disable=too-many-arguments self , path : str , refresh : bool = False , max_items : int = 100 , delimiter : str = \"/\" , prefix : str = \"\" , ): norm_path = path . strip ( \"/\" ) if norm_path in self . dircache and not refresh and not prefix and delimiter : return self . dircache [ norm_path ] logger . debug ( \"Get directory listing for %s \" , norm_path ) bucket , key = self . split_path ( norm_path ) prefix = prefix or \"\" if key : prefix = f \" { key } / { prefix } \" files = [] async for obj_dict in self . _iterdir ( bucket , max_keys = max_items , delimiter = delimiter , prefix = prefix , ): files . append ( obj_dict ) if not prefix and delimiter == \"/\" : self . dircache [ norm_path ] = files return files async def _iterdir ( self , bucket : str , max_keys : int = 100 , delimiter : str = \"/\" , prefix : str = \"\" , ): \"\"\"Iterate asynchronously over files and directories under `prefix`. The contents are yielded in arbitrary order as info dicts. \"\"\" response = await self . _call_oss ( \"AioObjectIterator\" , bucket = bucket , prefix = prefix , delimiter = delimiter , max_keys = max_keys , ) obj : \"SimplifiedObjectInfo\" async for obj in response : data = self . _transfer_object_info_to_dict ( bucket , obj ) yield data async def _ls_buckets ( self , refresh : bool = False ) -> List [ Dict [ str , Any ]]: if \"\" not in self . dircache or refresh : if isinstance ( self . _auth , AnonymousAuth ): logging . warning ( \"cannot list buckets if not logged in\" ) return [] results : List [ Dict [ str , Any ]] = [] try : files : \"ListBucketsResult\" = await self . _call_oss ( \"list_buckets\" ) except ClientError : # listbucket permission missing return [] file : \"SimplifiedBucketInfo\" for file in files . buckets : data : Dict [ str , Any ] = {} data [ \"name\" ] = file . name data [ \"size\" ] = 0 data [ \"type\" ] = \"directory\" results . append ( data ) self . dircache [ \"\" ] = results else : results = self . dircache [ \"\" ] return results @async_prettify_info_result async def _ls ( self , path : str , detail : bool = True , ** kwargs ): \"\"\"List files in given bucket, or list of buckets. Listing is cached unless `refresh=True`. Note: only your buckets associated with the login will be listed by `ls('')`, not any public buckets (even if already accessed). Parameters ---------- path : string/bytes location at which to list files refresh : bool (=False) if False, look in local cache for file details first \"\"\" refresh = kwargs . pop ( \"refresh\" , False ) norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) if norm_path != \"\" : files = await self . _ls_dir ( path , refresh ) if not files and \"/\" in norm_path : files = await self . _ls_dir ( self . _parent ( path ), refresh = refresh ) files = [ file for file in files if file [ \"name\" ] . strip ( \"/\" ) == norm_path and file [ \"type\" ] != \"directory\" ] else : files = await self . _ls_buckets ( refresh ) return files @async_prettify_info_result async def _info ( self , path : str , ** kwargs ): norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : result = { \"name\" : path , \"size\" : 0 , \"type\" : \"directory\" } return result bucket , key = self . split_path ( norm_path ) self . _get_bucket ( bucket ) refresh = kwargs . pop ( \"refresh\" , False ) if not refresh : out = self . _ls_from_cache ( norm_path ) if out is not None : out = [ o for o in out if o [ \"name\" ] . strip ( \"/\" ) == norm_path ] if out : result = out [ 0 ] else : result = { \"name\" : norm_path , \"size\" : 0 , \"type\" : \"directory\" } return result if key : try : obj_out : \"HeadObjectResult\" = await self . _call_oss ( \"head_object\" , key , bucket = bucket , ) result = { \"LastModified\" : obj_out . last_modified , \"size\" : obj_out . content_length , \"name\" : path , \"type\" : \"file\" , } return result except ( PermissionError , FileNotFoundError ): pass # We check to see if the path is a directory by attempting to list its # contexts. If anything is found, it is indeed a directory try : ls_out = await self . _call_oss ( \"AioObjectIterator\" , bucket = bucket , prefix = key . rstrip ( \"/\" ) + \"/\" , delimiter = \"/\" , max_keys = 100 , ) try : async for _ in ls_out : return { \"size\" : 0 , \"name\" : path , \"type\" : \"directory\" , } except OssError as err : raise translate_oss_error ( err ) from err except ( PermissionError , FileNotFoundError ): pass else : for bucket_info in await self . _ls_buckets (): if bucket_info [ \"name\" ] == norm_path . rstrip ( \"/\" ): return { \"size\" : 0 , \"name\" : path , \"type\" : \"directory\" , } raise FileNotFoundError ( path ) def _cache_result_analysis ( self , norm_path : str , parent : str ) -> bool : if norm_path in self . dircache : for file_info in self . dircache [ norm_path ]: # For files the dircache can contain itself. # If it contains anything other than itself it is a directory. if file_info [ \"name\" ] != norm_path : return True return False for file_info in self . dircache [ parent ]: if file_info [ \"name\" ] == norm_path : # If we find ourselves return whether we are a directory return file_info [ \"type\" ] == \"directory\" return False async def _isdir ( self , path : str ) -> bool : norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) # Send buckets to super if norm_path == \"\" : return True if \"/\" not in norm_path : for bucket_info in await self . _ls_buckets (): if bucket_info [ \"name\" ] == norm_path : return True return False parent = self . _parent ( norm_path ) if norm_path in self . dircache or parent in self . dircache : return self . _cache_result_analysis ( norm_path , parent ) # This only returns things within the path and NOT the path object itself try : return bool ( await self . _ls_dir ( norm_path )) except FileNotFoundError : return False async def _put_file ( self , lpath : str , rpath : str , ** kwargs ): bucket , key = self . split_path ( rpath ) if os . path . isdir ( lpath ): if key : # don't make remote \"directory\" return await self . _mkdir ( lpath ) else : callback = as_progress_handler ( kwargs . get ( \"callback\" , None )) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : await self . _call_oss ( \"resumable_upload\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) else : await self . _call_oss ( \"put_object_from_file\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) self . invalidate_cache ( self . _parent ( rpath )) async def _get_file ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote file to local \"\"\" bucket , key = self . split_path ( rpath ) if await self . _isdir ( rpath ): # don't make local \"directory\" return callback = as_progress_handler ( kwargs . get ( \"callback\" , None )) if await self . _size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : await self . _call_oss ( \"resumable_download\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ) else : await self . _call_oss ( \"get_object_to_file\" , bucket = bucket , key = key , filename = lpath , progress_callback = callback , ** kwargs , ) @async_prettify_info_result async def _find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , # pylint: disable=unused-argument ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. prefix: str Only return files that match ``^{path}/{prefix}`` (if there is an exact match ``filename == {path}/{prefix}``, it also will be included) \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : for info in await self . _ls_dir ( path , delimiter = \"\" , prefix = prefix ): out . update ({ info [ \"name\" ]: info }) else : async for _ , dirs , files in self . _walk ( path , maxdepth , detail = True ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for _ , info in files . items ()}) if await self . _isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names } async def _bulk_delete ( self , pathlist , ** kwargs ): \"\"\" Remove multiple keys with one call Parameters ---------- pathlist : list(str) The keys to remove, must all be in the same bucket. Must have 0 < len <= 1000 \"\"\" if not pathlist : return bucket , key_list = self . _get_batch_delete_key_list ( pathlist ) await self . _call_oss ( \"batch_delete_objects\" , key_list , bucket = bucket ) async def _rm_file ( self , path : str , ** kwargs ): bucket , key = self . split_path ( path ) await self . _call_oss ( \"delete_object\" , bucket = bucket , key = key ) self . invalidate_cache ( self . _parent ( path )) async def _rm ( self , path , recursive = False , batch_size = 1000 , ** kwargs ): if isinstance ( path , list ): for file in path : await self . _rm ( file ) return paths = await self . _expand_path ( path , recursive = recursive ) await _run_coros_in_chunks ( [ self . _bulk_delete ( paths [ i : i + batch_size ]) for i in range ( 0 , len ( paths ), batch_size ) ], batch_size = 3 , nofiles = True , ) async def _checksum ( self , path , refresh = True ): \"\"\" Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. Parameters ---------- path : string/bytes path of file to get checksum for refresh : bool (=False) if False, look in local cache for file details first \"\"\" return sha256 ( ( str ( await self . _ukey ( path )) + str ( await self . _info ( path , refresh = refresh )) ) . encode () ) . hexdigest () checksum = sync_wrapper ( _checksum ) async def _ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = await self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc checksum = sync_wrapper ( _checksum ) async def _cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\"Copy file between locations on OSS. preserve_etag: bool Whether to preserve etag while copying. If the file is uploaded as a single part, then it will be always equalivent to the md5 hash of the file hence etag will always be preserved. But if the file is uploaded in multi parts, then this option will try to reproduce the same multipart upload while copying and preserve the generated etag. \"\"\" bucket2 , key2 = self . split_path ( path2 ) bucket1 , key1 = self . split_path ( path1 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket1 != bucket2 : tempdir = \".\" + self . ukey ( path1 ) await self . _get_file ( path1 , tempdir , ** kwargs ) await self . _put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) await self . _call_oss ( \"copy_object\" , bucket1 , key1 , key2 , bucket = bucket1 , timeout = connect_timeout , ) async def _append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket , key = self . split_path ( path ) result : \"AppendObjectResult\" = await self . _call_oss ( \"append_object\" , key , location , value , bucket = bucket , ) return result . next_position append_object = sync_wrapper ( _append_object ) async def _get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket , key = self . split_path ( path ) object_stream : \"AioGetObjectResult\" = await self . _call_oss ( \"get_object\" , key , bucket = bucket , byte_range = ( start , end ), headers = headers , ) results = b \"\" while True : result = await object_stream . read () if result : results += result else : break return results get_object = sync_wrapper ( _get_object ) async def _pipe_file ( self , path : str , value : Union [ str , bytes ], ** kwargs ): bucket , key = self . split_path ( path ) self . invalidate_cache ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): await self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return init_multi_part_upload_result : \"InitMultipartUploadResult\" = ( await self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): part_number = i + 1 value_block = value [ off : off + block_size ] put_object_result : \"PutObjectResult\" = await self . _call_oss ( \"upload_part\" , key , init_multi_part_upload_result . upload_id , part_number , value_block , bucket = bucket , ) parts . append ( PartInfo ( part_number , put_object_result . etag , size = len ( value_block ), part_crc = put_object_result . crc , ) ) await self . _call_oss ( \"complete_multipart_upload\" , key , init_multi_part_upload_result . upload_id , parts , bucket = bucket , ) async def _cat_file ( self , path : str , start = None , end = None , ** kwargs ): bucket , key = self . split_path ( path ) object_stream : \"AioGetObjectResult\" = await self . _call_oss ( \"get_object\" , bucket = bucket , key = key , byte_range = ( start , end ), ** kwargs , ) results = b \"\" while True : result = await object_stream . read () if not result : break results += result return results async def _modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or await self . _isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) object_meta = await self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( object_meta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () ) modified = sync_wrapper ( _modified )","title":"Examples"},{"location":"reference/ossfs/async_oss/#ossfs.async_oss.AioOSSFileSystem.__init__","text":"Addition arguments Parameters: Name Type Description Default psize int concurrency number of the connections to DEFAULT_POOL_SIZE Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , psize : int = DEFAULT_POOL_SIZE , ** kwargs , ): \"\"\" ---------------------------------------------------------------- Addition arguments Args: psize (int, optional): concurrency number of the connections to the server. Defaults to DEFAULT_POOL_SIZE. \"\"\" super () . __init__ ( ** kwargs ) self . _psize = psize self . _session : Optional [ \"AioSession\" ] = None","title":"__init__()"},{"location":"reference/ossfs/async_oss/#ossfs.async_oss.AioOSSFileSystem.close_session","text":"Close a connection session object. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 122 123 124 125 126 127 128 129 130 131 def close_session ( self ): \"\"\"Close a connection session object.\"\"\" if self . _session is None or self . _session . closed : return if self . loop is not None and self . loop . is_running (): try : sync ( self . loop , self . _session . close , timeout = 0.1 ) return except FSTimeoutError : pass","title":"close_session()"},{"location":"reference/ossfs/async_oss/#ossfs.async_oss.AioOSSFileSystem.set_session","text":"Establish a connection session object. Returns Session to be closed later with await .close() Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/async_oss.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 async def set_session ( self , refresh : bool = False ): \"\"\"Establish a connection session object. Returns ------- Session to be closed later with await .close() \"\"\" logger . debug ( \"Connect AioSession instance\" ) if self . _session is None or self . _session . closed or refresh : if self . _session is None : self . _session = AioSession ( self . _psize ) await self . _session . __aenter__ () # pylint: disable=unnecessary-dunder-call # the following actually closes the aiohttp connection; use of privates # might break in the future, would cause exception at gc time if not self . asynchronous : weakref . finalize ( self , self . close_session ) return","title":"set_session()"},{"location":"reference/ossfs/base/","text":"Code of base class of OSSFileSystem BaseOSSFileSystem Bases: AbstractFileSystem base class of the ossfs (ossfs) OSS file system access OSS(Object Storage Service) as if it were a file system. This exposes a filesystem-like API (ls, cp, open, etc.) on top of OSS storage. Provide credentials with key and secret , or together with token . If anonymous leave all these argument empty. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/base.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 class BaseOSSFileSystem ( AbstractFileSystem ): # pylint: disable=abstract-method \"\"\" base class of the ossfs (ossfs) OSS file system access OSS(Object Storage Service) as if it were a file system. This exposes a filesystem-like API (ls, cp, open, etc.) on top of OSS storage. Provide credentials with `key` and `secret`, or together with `token`. If anonymous leave all these argument empty. \"\"\" protocol = \"oss\" def __init__ ( self , endpoint : Optional [ str ] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , token : Optional [ str ] = None , default_cache_type : str = \"readahead\" , default_block_size : Optional [ int ] = None , ** kwargs , # pylint: disable=too-many-arguments ): \"\"\" Parameters ---------- endpoint: string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or https://oss-me-east-1.aliyuncs.com, Can be changed after the initialization. key : string (None) If not anonymous, use this access key ID, if specified. secret : string (None) If not anonymous, use this secret access key, if specified. token : string (None) If not anonymous, use this security token, if specified. default_block_size: int (None) If given, the default block size value used for ``open()``, if no specific value is given at all time. The built-in default is 5MB. default_cache_type : string (\"readahead\") If given, the default cache_type value used for ``open()``. Set to \"none\" if no caching is desired. See fsspec's documentation for other available cache_type values. Default cache_type is \"readahead\". The following parameters are passed on to fsspec: skip_instance_cache: to control reuse of instances use_listings_cache, listings_expiry_time, max_paths: to control reuse of directory listings \"\"\" if token : self . _auth = StsAuth ( key , secret , token ) elif key : self . _auth = Auth ( key , secret ) else : self . _auth = AnonymousAuth () self . _endpoint = endpoint or os . getenv ( \"OSS_ENDPOINT\" ) if self . _endpoint is None : logger . warning ( \"OSS endpoint is not set, OSSFS could not work properly\" \"without a endpoint, please set it manually with \" \"`ossfs.set_endpoint` later\" ) super_kwargs = { k : kwargs . pop ( k ) for k in [ \"use_listings_cache\" , \"listings_expiry_time\" , \"max_paths\" ] if k in kwargs } # passed to fsspec superclass super () . __init__ ( ** super_kwargs ) self . _default_block_size = default_block_size or DEFAULT_BLOCK_SIZE self . _default_cache_type = default_cache_type def set_endpoint ( self , endpoint : str ): \"\"\" Reset the endpoint for ossfs endpoint : string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or \"\"\" if not endpoint : raise ValueError ( \"Not a valid endpoint\" ) self . _endpoint = endpoint @classmethod def _strip_protocol ( cls , path ): \"\"\"Turn path from fully-qualified to file-system-specifi Parameters ---------- path : Union[str, List[str]] Input path, like `http://oss-cn-hangzhou.aliyuncs.com/mybucket/myobject` `oss://mybucket/myobject` Examples -------- >>> _strip_protocol( \"http://oss-cn-hangzhou.aliyuncs.com/mybucket/myobject\" ) ('/mybucket/myobject') >>> _strip_protocol( \"oss://mybucket/myobject\" ) ('/mybucket/myobject') \"\"\" if isinstance ( path , list ): return [ cls . _strip_protocol ( p ) for p in path ] path_string : str = stringify_path ( path ) if path_string . startswith ( \"oss://\" ): path_string = path_string [ 5 :] parser_re = r \"https?://(?P<endpoint>oss.+aliyuncs\\.com)(?P<path>/.+)\" matcher = re . compile ( parser_re ) . match ( path_string ) if matcher : path_string = matcher [ \"path\" ] return path_string or cls . root_marker def split_path ( self , path : str ) -> Tuple [ str , str ]: \"\"\" Normalise object path string into bucket and key. Parameters ---------- path : string Input path, like `/mybucket/path/to/file` Examples -------- >>> split_path(\"/mybucket/path/to/file\") ['mybucket', 'path/to/file' ] >>> split_path(\" /mybucket/path/to/versioned_file?versionId=some_version_id \") ['mybucket', 'path/to/versioned_file', 'some_version_id'] \"\"\" path = self . _strip_protocol ( path ) path = path . lstrip ( \"/\" ) if \"/\" not in path : return path , \"\" bucket_name , obj_name = path . split ( \"/\" , 1 ) return bucket_name , obj_name def invalidate_cache ( self , path : Optional [ str ] = None ): if path is None : self . dircache . clear () else : norm_path : str = self . _strip_protocol ( path ) norm_path = norm_path . lstrip ( \"/\" ) self . dircache . pop ( norm_path , None ) while norm_path : self . dircache . pop ( norm_path , None ) norm_path = self . _parent ( norm_path ) def _transfer_object_info_to_dict ( self , bucket : str , obj : \"SimplifiedObjectInfo\" ) -> Dict : data : Dict [ str , Any ] = { \"name\" : \"/\" . join ([ bucket , obj . key ]), \"type\" : \"file\" , \"size\" : obj . size , } if obj . last_modified : data [ \"LastModified\" ] = obj . last_modified if obj . is_prefix (): data [ \"type\" ] = \"directory\" data [ \"size\" ] = 0 return data def _verify_find_arguments ( self , path : str , maxdepth : Optional [ int ], withdirs : bool , prefix : str ) -> str : path = self . _strip_protocol ( path ) bucket , _ = self . split_path ( path ) if not bucket : raise ValueError ( \"Cannot traverse all of the buckets\" ) if ( withdirs or maxdepth ) and prefix : raise ValueError ( \"Can not specify 'prefix' option alongside \" \"'withdirs'/'maxdepth' options.\" ) return path def _get_batch_delete_key_list ( self , pathlist : List [ str ]) -> Tuple [ str , List [ str ]]: buckets : Set [ str ] = set () key_list : List [ str ] = [] for path in pathlist : bucket , key = self . split_path ( path ) buckets . add ( bucket ) key_list . append ( key ) if len ( buckets ) > 1 : raise ValueError ( \"Bulk delete files should refer to only one bucket\" ) bucket = buckets . pop () if len ( pathlist ) > 1000 : raise ValueError ( \"Max number of files to delete in one call is 1000\" ) for path in pathlist : self . invalidate_cache ( self . _parent ( path )) return bucket , key_list def _open ( self , path : str , mode : str = \"rb\" , block_size : Optional [ int ] = None , autocommit : bool = True , cache_options : Optional [ str ] = None , ** kwargs , # pylint: disable=too-many-arguments ) -> \"OSSFile\" : \"\"\" Open a file for reading or writing. Parameters ---------- path: str File location mode: str 'rb', 'wb', etc. autocommit: bool If False, writes to temporary file that only gets put in final location upon commit kwargs Returns ------- OSSFile instance \"\"\" cache_type = kwargs . pop ( \"cache_type\" , self . _default_cache_type ) return OSSFile ( self , path , mode , block_size , autocommit , cache_options = cache_options , cache_type = cache_type , ** kwargs , ) def touch ( self , path : str , truncate : bool = True , ** kwargs ): \"\"\"Create empty file, or update timestamp Parameters ---------- path: str file location truncate: bool If True, always set file size to 0; if False, update timestamp and leave file unchanged, if backend allows this \"\"\" if truncate or not self . exists ( path ): self . invalidate_cache ( self . _parent ( path )) with self . open ( path , \"wb\" , ** kwargs ): pass else : raise NotImplementedError __init__ ( endpoint = None , key = None , secret = None , token = None , default_cache_type = 'readahead' , default_block_size = None , kwargs ) Parameters string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or https://oss-me-east-1.aliyuncs.com, Can be changed after the initialization. string (None) If not anonymous, use this access key ID, if specified. string (None) If not anonymous, use this secret access key, if specified. string (None) If not anonymous, use this security token, if specified. int (None) If given, the default block size value used for open() , if no specific value is given at all time. The built-in default is 5MB. string (\"readahead\") If given, the default cache_type value used for open() . Set to \"none\" if no caching is desired. See fsspec's documentation for other available cache_type values. Default cache_type is \"readahead\". The following parameters are passed on to fsspec: skip_instance_cache: to control reuse of instances use_listings_cache, listings_expiry_time, max_paths: to control reuse of directory listings Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/base.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , endpoint : Optional [ str ] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , token : Optional [ str ] = None , default_cache_type : str = \"readahead\" , default_block_size : Optional [ int ] = None , ** kwargs , # pylint: disable=too-many-arguments ): \"\"\" Parameters ---------- endpoint: string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or https://oss-me-east-1.aliyuncs.com, Can be changed after the initialization. key : string (None) If not anonymous, use this access key ID, if specified. secret : string (None) If not anonymous, use this secret access key, if specified. token : string (None) If not anonymous, use this security token, if specified. default_block_size: int (None) If given, the default block size value used for ``open()``, if no specific value is given at all time. The built-in default is 5MB. default_cache_type : string (\"readahead\") If given, the default cache_type value used for ``open()``. Set to \"none\" if no caching is desired. See fsspec's documentation for other available cache_type values. Default cache_type is \"readahead\". The following parameters are passed on to fsspec: skip_instance_cache: to control reuse of instances use_listings_cache, listings_expiry_time, max_paths: to control reuse of directory listings \"\"\" if token : self . _auth = StsAuth ( key , secret , token ) elif key : self . _auth = Auth ( key , secret ) else : self . _auth = AnonymousAuth () self . _endpoint = endpoint or os . getenv ( \"OSS_ENDPOINT\" ) if self . _endpoint is None : logger . warning ( \"OSS endpoint is not set, OSSFS could not work properly\" \"without a endpoint, please set it manually with \" \"`ossfs.set_endpoint` later\" ) super_kwargs = { k : kwargs . pop ( k ) for k in [ \"use_listings_cache\" , \"listings_expiry_time\" , \"max_paths\" ] if k in kwargs } # passed to fsspec superclass super () . __init__ ( ** super_kwargs ) self . _default_block_size = default_block_size or DEFAULT_BLOCK_SIZE self . _default_cache_type = default_cache_type set_endpoint ( endpoint ) Reset the endpoint for ossfs string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/base.py 108 109 110 111 112 113 114 115 116 117 118 def set_endpoint ( self , endpoint : str ): \"\"\" Reset the endpoint for ossfs endpoint : string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or \"\"\" if not endpoint : raise ValueError ( \"Not a valid endpoint\" ) self . _endpoint = endpoint split_path ( path ) Normalise object path string into bucket and key. Parameters string Input path, like /mybucket/path/to/file Examples split_path(\"/mybucket/path/to/file\") ['mybucket', 'path/to/file' ] split_path(\" /mybucket/path/to/versioned_file?versionId=some_version_id \") ['mybucket', 'path/to/versioned_file', 'some_version_id'] Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/base.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def split_path ( self , path : str ) -> Tuple [ str , str ]: \"\"\" Normalise object path string into bucket and key. Parameters ---------- path : string Input path, like `/mybucket/path/to/file` Examples -------- >>> split_path(\"/mybucket/path/to/file\") ['mybucket', 'path/to/file' ] >>> split_path(\" /mybucket/path/to/versioned_file?versionId=some_version_id \") ['mybucket', 'path/to/versioned_file', 'some_version_id'] \"\"\" path = self . _strip_protocol ( path ) path = path . lstrip ( \"/\" ) if \"/\" not in path : return path , \"\" bucket_name , obj_name = path . split ( \"/\" , 1 ) return bucket_name , obj_name touch ( path , truncate = True , kwargs ) Create empty file, or update timestamp Parameters str file location bool If True, always set file size to 0; if False, update timestamp and leave file unchanged, if backend allows this Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/base.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def touch ( self , path : str , truncate : bool = True , ** kwargs ): \"\"\"Create empty file, or update timestamp Parameters ---------- path: str file location truncate: bool If True, always set file size to 0; if False, update timestamp and leave file unchanged, if backend allows this \"\"\" if truncate or not self . exists ( path ): self . invalidate_cache ( self . _parent ( path )) with self . open ( path , \"wb\" , ** kwargs ): pass else : raise NotImplementedError","title":"Base"},{"location":"reference/ossfs/base/#ossfs.base.BaseOSSFileSystem","text":"Bases: AbstractFileSystem base class of the ossfs (ossfs) OSS file system access OSS(Object Storage Service) as if it were a file system. This exposes a filesystem-like API (ls, cp, open, etc.) on top of OSS storage. Provide credentials with key and secret , or together with token . If anonymous leave all these argument empty. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/base.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 class BaseOSSFileSystem ( AbstractFileSystem ): # pylint: disable=abstract-method \"\"\" base class of the ossfs (ossfs) OSS file system access OSS(Object Storage Service) as if it were a file system. This exposes a filesystem-like API (ls, cp, open, etc.) on top of OSS storage. Provide credentials with `key` and `secret`, or together with `token`. If anonymous leave all these argument empty. \"\"\" protocol = \"oss\" def __init__ ( self , endpoint : Optional [ str ] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , token : Optional [ str ] = None , default_cache_type : str = \"readahead\" , default_block_size : Optional [ int ] = None , ** kwargs , # pylint: disable=too-many-arguments ): \"\"\" Parameters ---------- endpoint: string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or https://oss-me-east-1.aliyuncs.com, Can be changed after the initialization. key : string (None) If not anonymous, use this access key ID, if specified. secret : string (None) If not anonymous, use this secret access key, if specified. token : string (None) If not anonymous, use this security token, if specified. default_block_size: int (None) If given, the default block size value used for ``open()``, if no specific value is given at all time. The built-in default is 5MB. default_cache_type : string (\"readahead\") If given, the default cache_type value used for ``open()``. Set to \"none\" if no caching is desired. See fsspec's documentation for other available cache_type values. Default cache_type is \"readahead\". The following parameters are passed on to fsspec: skip_instance_cache: to control reuse of instances use_listings_cache, listings_expiry_time, max_paths: to control reuse of directory listings \"\"\" if token : self . _auth = StsAuth ( key , secret , token ) elif key : self . _auth = Auth ( key , secret ) else : self . _auth = AnonymousAuth () self . _endpoint = endpoint or os . getenv ( \"OSS_ENDPOINT\" ) if self . _endpoint is None : logger . warning ( \"OSS endpoint is not set, OSSFS could not work properly\" \"without a endpoint, please set it manually with \" \"`ossfs.set_endpoint` later\" ) super_kwargs = { k : kwargs . pop ( k ) for k in [ \"use_listings_cache\" , \"listings_expiry_time\" , \"max_paths\" ] if k in kwargs } # passed to fsspec superclass super () . __init__ ( ** super_kwargs ) self . _default_block_size = default_block_size or DEFAULT_BLOCK_SIZE self . _default_cache_type = default_cache_type def set_endpoint ( self , endpoint : str ): \"\"\" Reset the endpoint for ossfs endpoint : string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or \"\"\" if not endpoint : raise ValueError ( \"Not a valid endpoint\" ) self . _endpoint = endpoint @classmethod def _strip_protocol ( cls , path ): \"\"\"Turn path from fully-qualified to file-system-specifi Parameters ---------- path : Union[str, List[str]] Input path, like `http://oss-cn-hangzhou.aliyuncs.com/mybucket/myobject` `oss://mybucket/myobject` Examples -------- >>> _strip_protocol( \"http://oss-cn-hangzhou.aliyuncs.com/mybucket/myobject\" ) ('/mybucket/myobject') >>> _strip_protocol( \"oss://mybucket/myobject\" ) ('/mybucket/myobject') \"\"\" if isinstance ( path , list ): return [ cls . _strip_protocol ( p ) for p in path ] path_string : str = stringify_path ( path ) if path_string . startswith ( \"oss://\" ): path_string = path_string [ 5 :] parser_re = r \"https?://(?P<endpoint>oss.+aliyuncs\\.com)(?P<path>/.+)\" matcher = re . compile ( parser_re ) . match ( path_string ) if matcher : path_string = matcher [ \"path\" ] return path_string or cls . root_marker def split_path ( self , path : str ) -> Tuple [ str , str ]: \"\"\" Normalise object path string into bucket and key. Parameters ---------- path : string Input path, like `/mybucket/path/to/file` Examples -------- >>> split_path(\"/mybucket/path/to/file\") ['mybucket', 'path/to/file' ] >>> split_path(\" /mybucket/path/to/versioned_file?versionId=some_version_id \") ['mybucket', 'path/to/versioned_file', 'some_version_id'] \"\"\" path = self . _strip_protocol ( path ) path = path . lstrip ( \"/\" ) if \"/\" not in path : return path , \"\" bucket_name , obj_name = path . split ( \"/\" , 1 ) return bucket_name , obj_name def invalidate_cache ( self , path : Optional [ str ] = None ): if path is None : self . dircache . clear () else : norm_path : str = self . _strip_protocol ( path ) norm_path = norm_path . lstrip ( \"/\" ) self . dircache . pop ( norm_path , None ) while norm_path : self . dircache . pop ( norm_path , None ) norm_path = self . _parent ( norm_path ) def _transfer_object_info_to_dict ( self , bucket : str , obj : \"SimplifiedObjectInfo\" ) -> Dict : data : Dict [ str , Any ] = { \"name\" : \"/\" . join ([ bucket , obj . key ]), \"type\" : \"file\" , \"size\" : obj . size , } if obj . last_modified : data [ \"LastModified\" ] = obj . last_modified if obj . is_prefix (): data [ \"type\" ] = \"directory\" data [ \"size\" ] = 0 return data def _verify_find_arguments ( self , path : str , maxdepth : Optional [ int ], withdirs : bool , prefix : str ) -> str : path = self . _strip_protocol ( path ) bucket , _ = self . split_path ( path ) if not bucket : raise ValueError ( \"Cannot traverse all of the buckets\" ) if ( withdirs or maxdepth ) and prefix : raise ValueError ( \"Can not specify 'prefix' option alongside \" \"'withdirs'/'maxdepth' options.\" ) return path def _get_batch_delete_key_list ( self , pathlist : List [ str ]) -> Tuple [ str , List [ str ]]: buckets : Set [ str ] = set () key_list : List [ str ] = [] for path in pathlist : bucket , key = self . split_path ( path ) buckets . add ( bucket ) key_list . append ( key ) if len ( buckets ) > 1 : raise ValueError ( \"Bulk delete files should refer to only one bucket\" ) bucket = buckets . pop () if len ( pathlist ) > 1000 : raise ValueError ( \"Max number of files to delete in one call is 1000\" ) for path in pathlist : self . invalidate_cache ( self . _parent ( path )) return bucket , key_list def _open ( self , path : str , mode : str = \"rb\" , block_size : Optional [ int ] = None , autocommit : bool = True , cache_options : Optional [ str ] = None , ** kwargs , # pylint: disable=too-many-arguments ) -> \"OSSFile\" : \"\"\" Open a file for reading or writing. Parameters ---------- path: str File location mode: str 'rb', 'wb', etc. autocommit: bool If False, writes to temporary file that only gets put in final location upon commit kwargs Returns ------- OSSFile instance \"\"\" cache_type = kwargs . pop ( \"cache_type\" , self . _default_cache_type ) return OSSFile ( self , path , mode , block_size , autocommit , cache_options = cache_options , cache_type = cache_type , ** kwargs , ) def touch ( self , path : str , truncate : bool = True , ** kwargs ): \"\"\"Create empty file, or update timestamp Parameters ---------- path: str file location truncate: bool If True, always set file size to 0; if False, update timestamp and leave file unchanged, if backend allows this \"\"\" if truncate or not self . exists ( path ): self . invalidate_cache ( self . _parent ( path )) with self . open ( path , \"wb\" , ** kwargs ): pass else : raise NotImplementedError","title":"BaseOSSFileSystem"},{"location":"reference/ossfs/base/#ossfs.base.BaseOSSFileSystem.__init__","text":"","title":"__init__()"},{"location":"reference/ossfs/base/#ossfs.base.BaseOSSFileSystem.__init__--parameters","text":"string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or https://oss-me-east-1.aliyuncs.com, Can be changed after the initialization. string (None) If not anonymous, use this access key ID, if specified. string (None) If not anonymous, use this secret access key, if specified. string (None) If not anonymous, use this security token, if specified. int (None) If given, the default block size value used for open() , if no specific value is given at all time. The built-in default is 5MB. string (\"readahead\") If given, the default cache_type value used for open() . Set to \"none\" if no caching is desired. See fsspec's documentation for other available cache_type values. Default cache_type is \"readahead\". The following parameters are passed on to fsspec: skip_instance_cache: to control reuse of instances use_listings_cache, listings_expiry_time, max_paths: to control reuse of directory listings Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/base.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , endpoint : Optional [ str ] = None , key : Optional [ str ] = None , secret : Optional [ str ] = None , token : Optional [ str ] = None , default_cache_type : str = \"readahead\" , default_block_size : Optional [ int ] = None , ** kwargs , # pylint: disable=too-many-arguments ): \"\"\" Parameters ---------- endpoint: string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or https://oss-me-east-1.aliyuncs.com, Can be changed after the initialization. key : string (None) If not anonymous, use this access key ID, if specified. secret : string (None) If not anonymous, use this secret access key, if specified. token : string (None) If not anonymous, use this security token, if specified. default_block_size: int (None) If given, the default block size value used for ``open()``, if no specific value is given at all time. The built-in default is 5MB. default_cache_type : string (\"readahead\") If given, the default cache_type value used for ``open()``. Set to \"none\" if no caching is desired. See fsspec's documentation for other available cache_type values. Default cache_type is \"readahead\". The following parameters are passed on to fsspec: skip_instance_cache: to control reuse of instances use_listings_cache, listings_expiry_time, max_paths: to control reuse of directory listings \"\"\" if token : self . _auth = StsAuth ( key , secret , token ) elif key : self . _auth = Auth ( key , secret ) else : self . _auth = AnonymousAuth () self . _endpoint = endpoint or os . getenv ( \"OSS_ENDPOINT\" ) if self . _endpoint is None : logger . warning ( \"OSS endpoint is not set, OSSFS could not work properly\" \"without a endpoint, please set it manually with \" \"`ossfs.set_endpoint` later\" ) super_kwargs = { k : kwargs . pop ( k ) for k in [ \"use_listings_cache\" , \"listings_expiry_time\" , \"max_paths\" ] if k in kwargs } # passed to fsspec superclass super () . __init__ ( ** super_kwargs ) self . _default_block_size = default_block_size or DEFAULT_BLOCK_SIZE self . _default_cache_type = default_cache_type","title":"Parameters"},{"location":"reference/ossfs/base/#ossfs.base.BaseOSSFileSystem.set_endpoint","text":"Reset the endpoint for ossfs string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/base.py 108 109 110 111 112 113 114 115 116 117 118 def set_endpoint ( self , endpoint : str ): \"\"\" Reset the endpoint for ossfs endpoint : string (None) Default endpoints of the fs Endpoints are the adderss where OSS locate like: http://oss-cn-hangzhou.aliyuncs.com or \"\"\" if not endpoint : raise ValueError ( \"Not a valid endpoint\" ) self . _endpoint = endpoint","title":"set_endpoint()"},{"location":"reference/ossfs/base/#ossfs.base.BaseOSSFileSystem.split_path","text":"Normalise object path string into bucket and key. Parameters string Input path, like /mybucket/path/to/file","title":"split_path()"},{"location":"reference/ossfs/base/#ossfs.base.BaseOSSFileSystem.split_path--examples","text":"split_path(\"/mybucket/path/to/file\") ['mybucket', 'path/to/file' ] split_path(\" /mybucket/path/to/versioned_file?versionId=some_version_id \") ['mybucket', 'path/to/versioned_file', 'some_version_id'] Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/base.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def split_path ( self , path : str ) -> Tuple [ str , str ]: \"\"\" Normalise object path string into bucket and key. Parameters ---------- path : string Input path, like `/mybucket/path/to/file` Examples -------- >>> split_path(\"/mybucket/path/to/file\") ['mybucket', 'path/to/file' ] >>> split_path(\" /mybucket/path/to/versioned_file?versionId=some_version_id \") ['mybucket', 'path/to/versioned_file', 'some_version_id'] \"\"\" path = self . _strip_protocol ( path ) path = path . lstrip ( \"/\" ) if \"/\" not in path : return path , \"\" bucket_name , obj_name = path . split ( \"/\" , 1 ) return bucket_name , obj_name","title":"Examples"},{"location":"reference/ossfs/base/#ossfs.base.BaseOSSFileSystem.touch","text":"Create empty file, or update timestamp","title":"touch()"},{"location":"reference/ossfs/base/#ossfs.base.BaseOSSFileSystem.touch--parameters","text":"str file location bool If True, always set file size to 0; if False, update timestamp and leave file unchanged, if backend allows this Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/base.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def touch ( self , path : str , truncate : bool = True , ** kwargs ): \"\"\"Create empty file, or update timestamp Parameters ---------- path: str file location truncate: bool If True, always set file size to 0; if False, update timestamp and leave file unchanged, if backend allows this \"\"\" if truncate or not self . exists ( path ): self . invalidate_cache ( self . _parent ( path )) with self . open ( path , \"wb\" , ** kwargs ): pass else : raise NotImplementedError","title":"Parameters"},{"location":"reference/ossfs/core/","text":"Code of OSSFileSystem OSSFileSystem Bases: BaseOSSFileSystem A pythonic file-systems interface to OSS (Object Storage Service) Examples ossfs = OSSFileSystem(anon=False) ossfs.ls('my-bucket/') ['my-file.txt'] with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 class OSSFileSystem ( BaseOSSFileSystem ): # pylint:disable=too-many-public-methods # pylint:disable=no-value-for-parameter \"\"\" A pythonic file-systems interface to OSS (Object Storage Service) Examples -------- >>> ossfs = OSSFileSystem(anon=False) >>> ossfs.ls('my-bucket/') ['my-file.txt'] >>> with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _session = oss2 . Session () def _get_bucket ( self , bucket_name : str , connect_timeout : Optional [ int ] = None ) -> oss2 . Bucket : \"\"\" get the new bucket instance \"\"\" if not self . _endpoint : raise ValueError ( \"endpoint is required\" ) try : return oss2 . Bucket ( self . _auth , self . _endpoint , bucket_name , session = self . _session , connect_timeout = connect_timeout , app_name = \"ossfs\" , ) except oss2 . exceptions . ClientError as err : raise FileNotFoundError ( bucket_name ) from err def _call_oss ( self , method_name : str , * args , bucket : Optional [ str ] = None , timeout : Optional [ int ] = None , retry : int = 3 , ** kwargs , ): if bucket : service = self . _get_bucket ( bucket , timeout ) else : service = oss2 . Service ( self . _auth , endpoint = self . _endpoint , connect_timeout = timeout , ) for count in range ( retry ): try : method = getattr ( service , method_name , None ) if not method : method = getattr ( oss2 , method_name ) logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( service , * args , ** kwargs ) else : logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( * args , ** kwargs ) return out except oss2 . exceptions . RequestError as err : logger . debug ( \"Retryable error: %s , try %s times\" , err , count + 1 ) error = err except oss2 . exceptions . OssError as err : logger . debug ( \"Nonretryable error: %s \" , err ) error = err break raise translate_oss_error ( error ) from error def _ls_bucket ( self , connect_timeout : Optional [ int ]) -> List [ Dict [ str , Any ]]: if \"\" not in self . dircache : results : List [ Dict [ str , Any ]] = [] if isinstance ( self . _auth , AnonymousAuth ): logging . warning ( \"cannot list buckets if not logged in\" ) return [] try : for bucket in self . _call_oss ( \"BucketIterator\" , timeout = connect_timeout ): result = { \"name\" : bucket . name , \"type\" : \"directory\" , \"size\" : 0 , \"CreateTime\" : bucket . creation_date , } results . append ( result ) except oss2 . exceptions . ClientError : pass self . dircache [ \"\" ] = copy . deepcopy ( results ) else : results = self . dircache [ \"\" ] return results def _get_object_info_list ( self , bucket_name : str , prefix : str , delimiter : str , connect_timeout : Optional [ int ], ): \"\"\" Wrap oss2.ObjectIterator return values into a fsspec form of file info \"\"\" result = [] obj : \"SimplifiedObjectInfo\" for obj in self . _call_oss ( \"ObjectIterator\" , prefix = prefix , delimiter = delimiter , bucket = bucket_name , timeout = connect_timeout , ): data = self . _transfer_object_info_to_dict ( bucket_name , obj ) result . append ( data ) return result def _ls_dir ( self , path : str , delimiter : str = \"/\" , refresh : bool = False , prefix : str = \"\" , connect_timeout : Optional [ int ] = None , ** kwargs , # pylint: disable=too-many-arguments ) -> List [ Dict ]: norm_path = path . strip ( \"/\" ) if norm_path in self . dircache and not refresh and not prefix and delimiter : return self . dircache [ norm_path ] logger . debug ( \"Get directory listing page for %s \" , norm_path ) bucket_name , key = self . split_path ( norm_path ) if not delimiter or prefix : if key : prefix = f \" { key } / { prefix } \" else : if norm_path in self . dircache and not refresh : return self . dircache [ norm_path ] if key : prefix = f \" { key } /\" try : self . dircache [ norm_path ] = self . _get_object_info_list ( bucket_name , prefix , delimiter , connect_timeout ) return self . dircache [ norm_path ] except oss2 . exceptions . AccessDenied : return [] @prettify_info_result def ls ( self , path : str , detail : bool = True , ** kwargs ): connect_timeout = kwargs . pop ( \"connect_timeout\" , 60 ) norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) if norm_path == \"\" : return self . _ls_bucket ( connect_timeout ) files = self . _ls_dir ( path , connect_timeout = connect_timeout ) if not files and \"/\" in norm_path : files = self . _ls_dir ( self . _parent ( path ), connect_timeout = connect_timeout ) files = [ file for file in files if file [ \"type\" ] != \"directory\" and file [ \"name\" ] . strip ( \"/\" ) == norm_path ] return files @prettify_info_result def find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ``ls``. \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : connect_timeout = kwargs . get ( \"connect_timeout\" , None ) for info in self . _ls_dir ( path , delimiter = \"\" , prefix = prefix , connect_timeout = connect_timeout ): out . update ({ info [ \"name\" ]: info }) else : for _ , dirs , files in self . walk ( path , maxdepth , detail = True , ** kwargs ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for name , info in files . items ()}) if self . isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names } def _directory_exists ( self , dirname : str , ** kwargs ): connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) ls_result = self . _ls_dir ( dirname , connect_timeout = connect_timeout ) return bool ( ls_result ) def _bucket_exist ( self , bucket_name : str ): if not bucket_name : return False try : self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) except ( oss2 . exceptions . OssError , PermissionError ): return False return True def exists ( self , path : str , ** kwargs ) -> bool : \"\"\"Is there a file at the given path\"\"\" norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : return True bucket_name , obj_name = self . split_path ( path ) if not self . _bucket_exist ( bucket_name ): return False connect_timeout = kwargs . get ( \"connect_timeout\" , None ) if not obj_name : return True if self . _call_oss ( \"object_exists\" , obj_name , bucket = bucket_name , timeout = connect_timeout , ): return True return self . _directory_exists ( path , ** kwargs ) def ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc def checksum ( self , path : str ): \"\"\"Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) \"\"\" return sha256 ( ( str ( self . ukey ( path )) + str ( self . info ( path ))) . encode () ) . hexdigest () def cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\" Copy within two locations in the filesystem # todo: big file optimization \"\"\" bucket_name1 , obj_name1 = self . split_path ( path1 ) bucket_name2 , obj_name2 = self . split_path ( path2 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket_name1 != bucket_name2 : tempdir = \".\" + self . ukey ( path1 ) self . get_file ( path1 , tempdir , ** kwargs ) self . put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) self . _call_oss ( \"copy_object\" , bucket_name1 , obj_name1 , obj_name2 , bucket = bucket_name1 , timeout = connect_timeout , ) def _rm ( self , path : Union [ str , List [ str ]]): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. \"\"\" if isinstance ( path , list ): for file in path : self . _rm ( file ) return bucket_name , obj_name = self . split_path ( path ) self . invalidate_cache ( self . _parent ( path )) self . _call_oss ( \"delete_object\" , obj_name , bucket = bucket_name ) def _bulk_delete ( self , pathlist , ** kwargs ): \"\"\" Remove multiple keys with one call Parameters ---------- pathlist : list(str) The keys to remove, must all be in the same bucket. Must have 0 < len <= 1000 \"\"\" if not pathlist : return bucket , key_list = self . _get_batch_delete_key_list ( pathlist ) self . _call_oss ( \"batch_delete_objects\" , key_list , bucket = bucket ) def rm ( self , path : Union [ str , List [ str ]], recursive = False , maxdepth = None ): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. recursive: bool If file(s) are directories, recursively delete contents and then also remove the directory maxdepth: int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. \"\"\" if isinstance ( path , list ): for file in path : self . rm ( file ) return path_expand = self . expand_path ( path , recursive = recursive , maxdepth = maxdepth ) def chunks ( lst : list , num : int ): for i in range ( 0 , len ( lst ), num ): yield lst [ i : i + num ] for files in chunks ( path_expand , 1000 ): self . _bulk_delete ( files ) def get_path ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote path to local \"\"\" if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) else : self . get_file ( rpath , lpath , ** kwargs ) def get_file ( self , rpath : str , lpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single remote file to local \"\"\" bucket_name , obj_name = self . split_path ( rpath ) kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) return connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if self . size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_download ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"get_object_to_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) def put_file ( self , lpath : str , rpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single file to remote \"\"\" kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) bucket_name , obj_name = self . split_path ( rpath ) if os . path . isdir ( lpath ): if obj_name : # don't make remote \"directory\" return self . mkdir ( lpath ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_upload ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"put_object_from_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) self . invalidate_cache ( self . _parent ( rpath )) def created ( self , path : str ): \"\"\"Return the created timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if obj_name : raise NotImplementedError ( \"OSS has no created timestamp\" ) bucket_info = self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) timestamp = bucket_info . creation_date return datetime . fromtimestamp ( timestamp ) def modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or self . isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) simplifiedmeta = self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( simplifiedmeta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () ) def append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket_name , obj_name = self . split_path ( path ) result = self . _call_oss ( \"append_object\" , obj_name , location , value , bucket = bucket_name , ) return result . next_position def get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket_name , obj_name = self . split_path ( path ) try : object_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name , byte_range = ( start , end ), headers = headers , ) except oss2 . exceptions . ServerError as err : raise err return object_stream . read () def sign ( self , path : str , expiration : int = 100 , ** kwargs ): raise NotImplementedError ( \"Sign is not implemented for this filesystem\" ) def pipe_file ( self , path : str , value : str , ** kwargs ): \"\"\"Set the bytes of given file\"\"\" bucket , key = self . split_path ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT self . invalidate_cache ( path ) if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return mpu : \"InitMultipartUploadResult\" = self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): data = value [ off : off + block_size ] part_number = i + 1 out : \"PutObjectResult\" = self . _call_oss ( \"upload_part\" , key , mpu . upload_id , part_number , data , bucket = bucket , ) parts . append ( PartInfo ( part_number , out . etag , size = len ( data ), part_crc = out . crc , ) ) self . _call_oss ( \"complete_multipart_upload\" , key , mpu . upload_id , parts , bucket = bucket , ) @prettify_info_result def info ( self , path , ** kwargs ): norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : result = { \"name\" : path , \"size\" : 0 , \"type\" : \"directory\" } else : result = super () . info ( path , ** kwargs ) if \"StorageClass\" in result : del result [ \"StorageClass\" ] if \"CreateTime\" in result : del result [ \"CreateTime\" ] return result def cat_file ( self , path : str , start : int = None , end : int = None , ** kwargs ): bucket , object_name = self . split_path ( path ) object_stream : \"GetObjectResult\" = self . _call_oss ( \"get_object\" , bucket = bucket , key = object_name , byte_range = ( start , end ), ** kwargs , ) return object_stream . read () append_object ( path , location , value ) Append bytes to the object Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 478 479 480 481 482 483 484 485 486 487 488 489 490 def append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket_name , obj_name = self . split_path ( path ) result = self . _call_oss ( \"append_object\" , obj_name , location , value , bucket = bucket_name , ) return result . next_position checksum ( path ) Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents might have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def checksum ( self , path : str ): \"\"\"Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) \"\"\" return sha256 ( ( str ( self . ukey ( path )) + str ( self . info ( path ))) . encode () ) . hexdigest () cp_file ( path1 , path2 , kwargs ) Copy within two locations in the filesystem todo: big file optimization Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\" Copy within two locations in the filesystem # todo: big file optimization \"\"\" bucket_name1 , obj_name1 = self . split_path ( path1 ) bucket_name2 , obj_name2 = self . split_path ( path2 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket_name1 != bucket_name2 : tempdir = \".\" + self . ukey ( path1 ) self . get_file ( path1 , tempdir , ** kwargs ) self . put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) self . _call_oss ( \"copy_object\" , bucket_name1 , obj_name1 , obj_name2 , bucket = bucket_name1 , timeout = connect_timeout , ) created ( path ) Return the created timestamp of a file as a datetime.datetime Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 456 457 458 459 460 461 462 463 def created ( self , path : str ): \"\"\"Return the created timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if obj_name : raise NotImplementedError ( \"OSS has no created timestamp\" ) bucket_info = self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) timestamp = bucket_info . creation_date return datetime . fromtimestamp ( timestamp ) exists ( path , kwargs ) Is there a file at the given path Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 def exists ( self , path : str , ** kwargs ) -> bool : \"\"\"Is there a file at the given path\"\"\" norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : return True bucket_name , obj_name = self . split_path ( path ) if not self . _bucket_exist ( bucket_name ): return False connect_timeout = kwargs . get ( \"connect_timeout\" , None ) if not obj_name : return True if self . _call_oss ( \"object_exists\" , obj_name , bucket = bucket_name , timeout = connect_timeout , ): return True return self . _directory_exists ( path , ** kwargs ) find ( path , maxdepth = None , withdirs = False , detail = False , kwargs ) List all files below path. Like posix find command without conditions Parameters path : str int or None If not None, the maximum number of levels to descend bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ls . Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @prettify_info_result def find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ``ls``. \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : connect_timeout = kwargs . get ( \"connect_timeout\" , None ) for info in self . _ls_dir ( path , delimiter = \"\" , prefix = prefix , connect_timeout = connect_timeout ): out . update ({ info [ \"name\" ]: info }) else : for _ , dirs , files in self . walk ( path , maxdepth , detail = True , ** kwargs ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for name , info in files . items ()}) if self . isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names } get_file ( rpath , lpath , callback = None , kwargs ) Copy single remote file to local Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def get_file ( self , rpath : str , lpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single remote file to local \"\"\" bucket_name , obj_name = self . split_path ( rpath ) kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) return connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if self . size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_download ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"get_object_to_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) get_object ( path , start , end ) Return object bytes in range Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket_name , obj_name = self . split_path ( path ) try : object_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name , byte_range = ( start , end ), headers = headers , ) except oss2 . exceptions . ServerError as err : raise err return object_stream . read () get_path ( rpath , lpath , kwargs ) Copy single remote path to local Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 393 394 395 396 397 398 399 400 def get_path ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote path to local \"\"\" if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) else : self . get_file ( rpath , lpath , ** kwargs ) modified ( path ) Return the modified timestamp of a file as a datetime.datetime Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 465 466 467 468 469 470 471 472 473 474 475 476 def modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or self . isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) simplifiedmeta = self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( simplifiedmeta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () ) pipe_file ( path , value , kwargs ) Set the bytes of given file Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 def pipe_file ( self , path : str , value : str , ** kwargs ): \"\"\"Set the bytes of given file\"\"\" bucket , key = self . split_path ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT self . invalidate_cache ( path ) if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return mpu : \"InitMultipartUploadResult\" = self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): data = value [ off : off + block_size ] part_number = i + 1 out : \"PutObjectResult\" = self . _call_oss ( \"upload_part\" , key , mpu . upload_id , part_number , data , bucket = bucket , ) parts . append ( PartInfo ( part_number , out . etag , size = len ( data ), part_crc = out . crc , ) ) self . _call_oss ( \"complete_multipart_upload\" , key , mpu . upload_id , parts , bucket = bucket , ) put_file ( lpath , rpath , callback = None , kwargs ) Copy single file to remote Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 def put_file ( self , lpath : str , rpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single file to remote \"\"\" kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) bucket_name , obj_name = self . split_path ( rpath ) if os . path . isdir ( lpath ): if obj_name : # don't make remote \"directory\" return self . mkdir ( lpath ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_upload ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"put_object_from_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) self . invalidate_cache ( self . _parent ( rpath )) rm ( path , recursive = False , maxdepth = None ) Delete files. Parameters str or list of str File(s) to delete. bool If file(s) are directories, recursively delete contents and then also remove the directory int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 def rm ( self , path : Union [ str , List [ str ]], recursive = False , maxdepth = None ): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. recursive: bool If file(s) are directories, recursively delete contents and then also remove the directory maxdepth: int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. \"\"\" if isinstance ( path , list ): for file in path : self . rm ( file ) return path_expand = self . expand_path ( path , recursive = recursive , maxdepth = maxdepth ) def chunks ( lst : list , num : int ): for i in range ( 0 , len ( lst ), num ): yield lst [ i : i + num ] for files in chunks ( path_expand , 1000 ): self . _bulk_delete ( files ) ukey ( path ) Hash of file properties, to tell if it has changed Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 287 288 289 290 291 def ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc","title":"Core"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem","text":"Bases: BaseOSSFileSystem A pythonic file-systems interface to OSS (Object Storage Service)","title":"OSSFileSystem"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem--examples","text":"ossfs = OSSFileSystem(anon=False) ossfs.ls('my-bucket/') ['my-file.txt'] with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 class OSSFileSystem ( BaseOSSFileSystem ): # pylint:disable=too-many-public-methods # pylint:disable=no-value-for-parameter \"\"\" A pythonic file-systems interface to OSS (Object Storage Service) Examples -------- >>> ossfs = OSSFileSystem(anon=False) >>> ossfs.ls('my-bucket/') ['my-file.txt'] >>> with ossfs.open('my-bucket/my-file.txt', mode='rb') as f: ... print(f.read()) b'Hello, world!' \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _session = oss2 . Session () def _get_bucket ( self , bucket_name : str , connect_timeout : Optional [ int ] = None ) -> oss2 . Bucket : \"\"\" get the new bucket instance \"\"\" if not self . _endpoint : raise ValueError ( \"endpoint is required\" ) try : return oss2 . Bucket ( self . _auth , self . _endpoint , bucket_name , session = self . _session , connect_timeout = connect_timeout , app_name = \"ossfs\" , ) except oss2 . exceptions . ClientError as err : raise FileNotFoundError ( bucket_name ) from err def _call_oss ( self , method_name : str , * args , bucket : Optional [ str ] = None , timeout : Optional [ int ] = None , retry : int = 3 , ** kwargs , ): if bucket : service = self . _get_bucket ( bucket , timeout ) else : service = oss2 . Service ( self . _auth , endpoint = self . _endpoint , connect_timeout = timeout , ) for count in range ( retry ): try : method = getattr ( service , method_name , None ) if not method : method = getattr ( oss2 , method_name ) logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( service , * args , ** kwargs ) else : logger . debug ( \"CALL: %s - %s - %s \" , method . __name__ , args , kwargs ) out = method ( * args , ** kwargs ) return out except oss2 . exceptions . RequestError as err : logger . debug ( \"Retryable error: %s , try %s times\" , err , count + 1 ) error = err except oss2 . exceptions . OssError as err : logger . debug ( \"Nonretryable error: %s \" , err ) error = err break raise translate_oss_error ( error ) from error def _ls_bucket ( self , connect_timeout : Optional [ int ]) -> List [ Dict [ str , Any ]]: if \"\" not in self . dircache : results : List [ Dict [ str , Any ]] = [] if isinstance ( self . _auth , AnonymousAuth ): logging . warning ( \"cannot list buckets if not logged in\" ) return [] try : for bucket in self . _call_oss ( \"BucketIterator\" , timeout = connect_timeout ): result = { \"name\" : bucket . name , \"type\" : \"directory\" , \"size\" : 0 , \"CreateTime\" : bucket . creation_date , } results . append ( result ) except oss2 . exceptions . ClientError : pass self . dircache [ \"\" ] = copy . deepcopy ( results ) else : results = self . dircache [ \"\" ] return results def _get_object_info_list ( self , bucket_name : str , prefix : str , delimiter : str , connect_timeout : Optional [ int ], ): \"\"\" Wrap oss2.ObjectIterator return values into a fsspec form of file info \"\"\" result = [] obj : \"SimplifiedObjectInfo\" for obj in self . _call_oss ( \"ObjectIterator\" , prefix = prefix , delimiter = delimiter , bucket = bucket_name , timeout = connect_timeout , ): data = self . _transfer_object_info_to_dict ( bucket_name , obj ) result . append ( data ) return result def _ls_dir ( self , path : str , delimiter : str = \"/\" , refresh : bool = False , prefix : str = \"\" , connect_timeout : Optional [ int ] = None , ** kwargs , # pylint: disable=too-many-arguments ) -> List [ Dict ]: norm_path = path . strip ( \"/\" ) if norm_path in self . dircache and not refresh and not prefix and delimiter : return self . dircache [ norm_path ] logger . debug ( \"Get directory listing page for %s \" , norm_path ) bucket_name , key = self . split_path ( norm_path ) if not delimiter or prefix : if key : prefix = f \" { key } / { prefix } \" else : if norm_path in self . dircache and not refresh : return self . dircache [ norm_path ] if key : prefix = f \" { key } /\" try : self . dircache [ norm_path ] = self . _get_object_info_list ( bucket_name , prefix , delimiter , connect_timeout ) return self . dircache [ norm_path ] except oss2 . exceptions . AccessDenied : return [] @prettify_info_result def ls ( self , path : str , detail : bool = True , ** kwargs ): connect_timeout = kwargs . pop ( \"connect_timeout\" , 60 ) norm_path = self . _strip_protocol ( path ) . strip ( \"/\" ) if norm_path == \"\" : return self . _ls_bucket ( connect_timeout ) files = self . _ls_dir ( path , connect_timeout = connect_timeout ) if not files and \"/\" in norm_path : files = self . _ls_dir ( self . _parent ( path ), connect_timeout = connect_timeout ) files = [ file for file in files if file [ \"type\" ] != \"directory\" and file [ \"name\" ] . strip ( \"/\" ) == norm_path ] return files @prettify_info_result def find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ``ls``. \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : connect_timeout = kwargs . get ( \"connect_timeout\" , None ) for info in self . _ls_dir ( path , delimiter = \"\" , prefix = prefix , connect_timeout = connect_timeout ): out . update ({ info [ \"name\" ]: info }) else : for _ , dirs , files in self . walk ( path , maxdepth , detail = True , ** kwargs ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for name , info in files . items ()}) if self . isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names } def _directory_exists ( self , dirname : str , ** kwargs ): connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) ls_result = self . _ls_dir ( dirname , connect_timeout = connect_timeout ) return bool ( ls_result ) def _bucket_exist ( self , bucket_name : str ): if not bucket_name : return False try : self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) except ( oss2 . exceptions . OssError , PermissionError ): return False return True def exists ( self , path : str , ** kwargs ) -> bool : \"\"\"Is there a file at the given path\"\"\" norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : return True bucket_name , obj_name = self . split_path ( path ) if not self . _bucket_exist ( bucket_name ): return False connect_timeout = kwargs . get ( \"connect_timeout\" , None ) if not obj_name : return True if self . _call_oss ( \"object_exists\" , obj_name , bucket = bucket_name , timeout = connect_timeout , ): return True return self . _directory_exists ( path , ** kwargs ) def ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc def checksum ( self , path : str ): \"\"\"Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) \"\"\" return sha256 ( ( str ( self . ukey ( path )) + str ( self . info ( path ))) . encode () ) . hexdigest () def cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\" Copy within two locations in the filesystem # todo: big file optimization \"\"\" bucket_name1 , obj_name1 = self . split_path ( path1 ) bucket_name2 , obj_name2 = self . split_path ( path2 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket_name1 != bucket_name2 : tempdir = \".\" + self . ukey ( path1 ) self . get_file ( path1 , tempdir , ** kwargs ) self . put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) self . _call_oss ( \"copy_object\" , bucket_name1 , obj_name1 , obj_name2 , bucket = bucket_name1 , timeout = connect_timeout , ) def _rm ( self , path : Union [ str , List [ str ]]): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. \"\"\" if isinstance ( path , list ): for file in path : self . _rm ( file ) return bucket_name , obj_name = self . split_path ( path ) self . invalidate_cache ( self . _parent ( path )) self . _call_oss ( \"delete_object\" , obj_name , bucket = bucket_name ) def _bulk_delete ( self , pathlist , ** kwargs ): \"\"\" Remove multiple keys with one call Parameters ---------- pathlist : list(str) The keys to remove, must all be in the same bucket. Must have 0 < len <= 1000 \"\"\" if not pathlist : return bucket , key_list = self . _get_batch_delete_key_list ( pathlist ) self . _call_oss ( \"batch_delete_objects\" , key_list , bucket = bucket ) def rm ( self , path : Union [ str , List [ str ]], recursive = False , maxdepth = None ): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. recursive: bool If file(s) are directories, recursively delete contents and then also remove the directory maxdepth: int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. \"\"\" if isinstance ( path , list ): for file in path : self . rm ( file ) return path_expand = self . expand_path ( path , recursive = recursive , maxdepth = maxdepth ) def chunks ( lst : list , num : int ): for i in range ( 0 , len ( lst ), num ): yield lst [ i : i + num ] for files in chunks ( path_expand , 1000 ): self . _bulk_delete ( files ) def get_path ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote path to local \"\"\" if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) else : self . get_file ( rpath , lpath , ** kwargs ) def get_file ( self , rpath : str , lpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single remote file to local \"\"\" bucket_name , obj_name = self . split_path ( rpath ) kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) return connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if self . size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_download ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"get_object_to_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) def put_file ( self , lpath : str , rpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single file to remote \"\"\" kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) bucket_name , obj_name = self . split_path ( rpath ) if os . path . isdir ( lpath ): if obj_name : # don't make remote \"directory\" return self . mkdir ( lpath ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_upload ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"put_object_from_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) self . invalidate_cache ( self . _parent ( rpath )) def created ( self , path : str ): \"\"\"Return the created timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if obj_name : raise NotImplementedError ( \"OSS has no created timestamp\" ) bucket_info = self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) timestamp = bucket_info . creation_date return datetime . fromtimestamp ( timestamp ) def modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or self . isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) simplifiedmeta = self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( simplifiedmeta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () ) def append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket_name , obj_name = self . split_path ( path ) result = self . _call_oss ( \"append_object\" , obj_name , location , value , bucket = bucket_name , ) return result . next_position def get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket_name , obj_name = self . split_path ( path ) try : object_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name , byte_range = ( start , end ), headers = headers , ) except oss2 . exceptions . ServerError as err : raise err return object_stream . read () def sign ( self , path : str , expiration : int = 100 , ** kwargs ): raise NotImplementedError ( \"Sign is not implemented for this filesystem\" ) def pipe_file ( self , path : str , value : str , ** kwargs ): \"\"\"Set the bytes of given file\"\"\" bucket , key = self . split_path ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT self . invalidate_cache ( path ) if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return mpu : \"InitMultipartUploadResult\" = self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): data = value [ off : off + block_size ] part_number = i + 1 out : \"PutObjectResult\" = self . _call_oss ( \"upload_part\" , key , mpu . upload_id , part_number , data , bucket = bucket , ) parts . append ( PartInfo ( part_number , out . etag , size = len ( data ), part_crc = out . crc , ) ) self . _call_oss ( \"complete_multipart_upload\" , key , mpu . upload_id , parts , bucket = bucket , ) @prettify_info_result def info ( self , path , ** kwargs ): norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : result = { \"name\" : path , \"size\" : 0 , \"type\" : \"directory\" } else : result = super () . info ( path , ** kwargs ) if \"StorageClass\" in result : del result [ \"StorageClass\" ] if \"CreateTime\" in result : del result [ \"CreateTime\" ] return result def cat_file ( self , path : str , start : int = None , end : int = None , ** kwargs ): bucket , object_name = self . split_path ( path ) object_stream : \"GetObjectResult\" = self . _call_oss ( \"get_object\" , bucket = bucket , key = object_name , byte_range = ( start , end ), ** kwargs , ) return object_stream . read ()","title":"Examples"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.append_object","text":"Append bytes to the object Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 478 479 480 481 482 483 484 485 486 487 488 489 490 def append_object ( self , path : str , location : int , value : bytes ) -> int : \"\"\" Append bytes to the object \"\"\" bucket_name , obj_name = self . split_path ( path ) result = self . _call_oss ( \"append_object\" , obj_name , location , value , bucket = bucket_name , ) return result . next_position","title":"append_object()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.checksum","text":"Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents might have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def checksum ( self , path : str ): \"\"\"Unique value for current version of file If the checksum is the same from one moment to another, the contents are guaranteed to be the same. If the checksum changes, the contents *might* have changed. This should normally be overridden; default will probably capture creation/modification timestamp (which would be good) or maybe access timestamp (which would be bad) \"\"\" return sha256 ( ( str ( self . ukey ( path )) + str ( self . info ( path ))) . encode () ) . hexdigest ()","title":"checksum()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.cp_file","text":"Copy within two locations in the filesystem","title":"cp_file()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.cp_file--todo-big-file-optimization","text":"Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def cp_file ( self , path1 : str , path2 : str , ** kwargs ): \"\"\" Copy within two locations in the filesystem # todo: big file optimization \"\"\" bucket_name1 , obj_name1 = self . split_path ( path1 ) bucket_name2 , obj_name2 = self . split_path ( path2 ) self . invalidate_cache ( self . _parent ( path2 )) if bucket_name1 != bucket_name2 : tempdir = \".\" + self . ukey ( path1 ) self . get_file ( path1 , tempdir , ** kwargs ) self . put_file ( tempdir , path2 , ** kwargs ) os . remove ( tempdir ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) self . _call_oss ( \"copy_object\" , bucket_name1 , obj_name1 , obj_name2 , bucket = bucket_name1 , timeout = connect_timeout , )","title":"todo: big file optimization"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.created","text":"Return the created timestamp of a file as a datetime.datetime Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 456 457 458 459 460 461 462 463 def created ( self , path : str ): \"\"\"Return the created timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if obj_name : raise NotImplementedError ( \"OSS has no created timestamp\" ) bucket_info = self . _call_oss ( \"get_bucket_info\" , bucket = bucket_name ) timestamp = bucket_info . creation_date return datetime . fromtimestamp ( timestamp )","title":"created()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.exists","text":"Is there a file at the given path Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 def exists ( self , path : str , ** kwargs ) -> bool : \"\"\"Is there a file at the given path\"\"\" norm_path = self . _strip_protocol ( path ) . lstrip ( \"/\" ) if norm_path == \"\" : return True bucket_name , obj_name = self . split_path ( path ) if not self . _bucket_exist ( bucket_name ): return False connect_timeout = kwargs . get ( \"connect_timeout\" , None ) if not obj_name : return True if self . _call_oss ( \"object_exists\" , obj_name , bucket = bucket_name , timeout = connect_timeout , ): return True return self . _directory_exists ( path , ** kwargs )","title":"exists()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.find","text":"List all files below path. Like posix find command without conditions","title":"find()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.find--parameters","text":"path : str int or None If not None, the maximum number of levels to descend bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ls . Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @prettify_info_result def find ( self , path : str , maxdepth : Optional [ int ] = None , withdirs : bool = False , detail : bool = False , ** kwargs , ): \"\"\"List all files below path. Like posix ``find`` command without conditions Parameters ---------- path : str maxdepth: int or None If not None, the maximum number of levels to descend withdirs: bool Whether to include directory paths in the output. This is True when used by glob, but users usually only want files. kwargs are passed to ``ls``. \"\"\" out = {} prefix = kwargs . pop ( \"prefix\" , \"\" ) path = self . _verify_find_arguments ( path , maxdepth , withdirs , prefix ) if prefix : connect_timeout = kwargs . get ( \"connect_timeout\" , None ) for info in self . _ls_dir ( path , delimiter = \"\" , prefix = prefix , connect_timeout = connect_timeout ): out . update ({ info [ \"name\" ]: info }) else : for _ , dirs , files in self . walk ( path , maxdepth , detail = True , ** kwargs ): if withdirs : files . update ( dirs ) out . update ({ info [ \"name\" ]: info for name , info in files . items ()}) if self . isfile ( path ) and path not in out : # walk works on directories, but find should also return [path] # when path happens to be a file out [ path ] = {} names = sorted ( out ) return { name : out [ name ] for name in names }","title":"Parameters"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.get_file","text":"Copy single remote file to local Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def get_file ( self , rpath : str , lpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single remote file to local \"\"\" bucket_name , obj_name = self . split_path ( rpath ) kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) return connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if self . size ( rpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_download ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"get_object_to_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , )","title":"get_file()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.get_object","text":"Return object bytes in range Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def get_object ( self , path : str , start : int , end : int ) -> bytes : \"\"\" Return object bytes in range \"\"\" headers = { \"x-oss-range-behavior\" : \"standard\" } bucket_name , obj_name = self . split_path ( path ) try : object_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name , byte_range = ( start , end ), headers = headers , ) except oss2 . exceptions . ServerError as err : raise err return object_stream . read ()","title":"get_object()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.get_path","text":"Copy single remote path to local Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 393 394 395 396 397 398 399 400 def get_path ( self , rpath : str , lpath : str , ** kwargs ): \"\"\" Copy single remote path to local \"\"\" if self . isdir ( rpath ): os . makedirs ( lpath , exist_ok = True ) else : self . get_file ( rpath , lpath , ** kwargs )","title":"get_path()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.modified","text":"Return the modified timestamp of a file as a datetime.datetime Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 465 466 467 468 469 470 471 472 473 474 475 476 def modified ( self , path : str ): \"\"\"Return the modified timestamp of a file as a datetime.datetime\"\"\" bucket_name , obj_name = self . split_path ( path ) if not obj_name or self . isdir ( path ): raise NotImplementedError ( \"bucket has no modified timestamp\" ) simplifiedmeta = self . _call_oss ( \"get_object_meta\" , obj_name , bucket = bucket_name ) return int ( datetime . strptime ( simplifiedmeta . headers [ \"Last-Modified\" ], \" %a , %d %b %Y %H:%M:%S %Z\" , ) . timestamp () )","title":"modified()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.pipe_file","text":"Set the bytes of given file Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 def pipe_file ( self , path : str , value : str , ** kwargs ): \"\"\"Set the bytes of given file\"\"\" bucket , key = self . split_path ( path ) block_size = kwargs . get ( \"block_size\" , DEFAULT_BLOCK_SIZE ) # 5 GB is the limit for an OSS PUT self . invalidate_cache ( path ) if len ( value ) < min ( 5 * 2 ** 30 , 2 * block_size ): self . _call_oss ( \"put_object\" , key , value , bucket = bucket , ** kwargs ) return mpu : \"InitMultipartUploadResult\" = self . _call_oss ( \"init_multipart_upload\" , key , bucket = bucket , ** kwargs ) parts : List [ \"PartInfo\" ] = [] for i , off in enumerate ( range ( 0 , len ( value ), block_size )): data = value [ off : off + block_size ] part_number = i + 1 out : \"PutObjectResult\" = self . _call_oss ( \"upload_part\" , key , mpu . upload_id , part_number , data , bucket = bucket , ) parts . append ( PartInfo ( part_number , out . etag , size = len ( data ), part_crc = out . crc , ) ) self . _call_oss ( \"complete_multipart_upload\" , key , mpu . upload_id , parts , bucket = bucket , )","title":"pipe_file()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.put_file","text":"Copy single file to remote Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 def put_file ( self , lpath : str , rpath : str , callback : Optional [ Callable ] = None , ** kwargs ): # pylint: disable=arguments-differ \"\"\" Copy single file to remote \"\"\" kwargs . setdefault ( \"progress_callback\" , as_progress_handler ( callback )) bucket_name , obj_name = self . split_path ( rpath ) if os . path . isdir ( lpath ): if obj_name : # don't make remote \"directory\" return self . mkdir ( lpath ) else : connect_timeout = kwargs . pop ( \"connect_timeout\" , None ) bucket = self . _get_bucket ( bucket_name , connect_timeout ) if os . path . getsize ( lpath ) >= SIMPLE_TRANSFER_THRESHOLD : oss2 . resumable_upload ( bucket , obj_name , lpath , ** kwargs ) else : self . _call_oss ( \"put_object_from_file\" , obj_name , lpath , bucket = bucket_name , timeout = connect_timeout , ** kwargs , ) self . invalidate_cache ( self . _parent ( rpath ))","title":"put_file()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.rm","text":"Delete files.","title":"rm()"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.rm--parameters","text":"str or list of str File(s) to delete. bool If file(s) are directories, recursively delete contents and then also remove the directory int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 def rm ( self , path : Union [ str , List [ str ]], recursive = False , maxdepth = None ): \"\"\"Delete files. Parameters ---------- path: str or list of str File(s) to delete. recursive: bool If file(s) are directories, recursively delete contents and then also remove the directory maxdepth: int or None Depth to pass to walk for finding files to delete, if recursive. If None, there will be no limit and infinite recursion may be possible. \"\"\" if isinstance ( path , list ): for file in path : self . rm ( file ) return path_expand = self . expand_path ( path , recursive = recursive , maxdepth = maxdepth ) def chunks ( lst : list , num : int ): for i in range ( 0 , len ( lst ), num ): yield lst [ i : i + num ] for files in chunks ( path_expand , 1000 ): self . _bulk_delete ( files )","title":"Parameters"},{"location":"reference/ossfs/core/#ossfs.core.OSSFileSystem.ukey","text":"Hash of file properties, to tell if it has changed Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/core.py 287 288 289 290 291 def ukey ( self , path : str ): \"\"\"Hash of file properties, to tell if it has changed\"\"\" bucket_name , obj_name = self . split_path ( path ) obj_stream = self . _call_oss ( \"get_object\" , obj_name , bucket = bucket_name ) return obj_stream . server_crc","title":"ukey()"},{"location":"reference/ossfs/exceptions/","text":"OSS error codes adapted into more natural Python ones. Adapted from https://error-center.alibabacloud.com/status/product/Oss?spm=a2c63.p38356.879954.5.3e172c31eo6sN9 translate_oss_error ( error , args , message = None , set_cause = True , kwargs ) Convert a ClientError exception into a Python one. Parameters oss2.exceptions.OssError The exception returned by the OSS Server. str An error message to use for the returned exception. If not given, the error message returned by the server is used instead. bool Whether to set the cause attribute to the previous exception if the exception is translated. args, *kwargs : Additional arguments to pass to the exception constructor, after the error message. Useful for passing the filename arguments to IOError . Returns An instantiated exception ready to be thrown. If the error code isn't recognized, an IOError with the original error message is returned. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/exceptions.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def translate_oss_error ( error : OssError , * args , message = None , set_cause = True , ** kwargs ) -> BaseException : \"\"\"Convert a ClientError exception into a Python one. Parameters ---------- error : oss2.exceptions.OssError The exception returned by the OSS Server. message : str An error message to use for the returned exception. If not given, the error message returned by the server is used instead. set_cause : bool Whether to set the __cause__ attribute to the previous exception if the exception is translated. *args, **kwargs : Additional arguments to pass to the exception constructor, after the error message. Useful for passing the filename arguments to ``IOError``. Returns ------- An instantiated exception ready to be thrown. If the error code isn't recognized, an IOError with the original error message is returned. \"\"\" if not isinstance ( error , OssError ): # not a oss error: return error code = error . __class__ . __name__ constructor = ERROR_CODE_TO_EXCEPTION . get ( code ) if constructor : if not message : message = error . message custom_exc = constructor ( message , * args , ** kwargs ) else : # No match found, wrap this in an IOError with the appropriate message. custom_exc = IOError ( errno . EIO , message or str ( error ), * args ) if set_cause : custom_exc . __cause__ = error return custom_exc","title":"Exceptions"},{"location":"reference/ossfs/exceptions/#ossfs.exceptions.translate_oss_error","text":"Convert a ClientError exception into a Python one. Parameters oss2.exceptions.OssError The exception returned by the OSS Server. str An error message to use for the returned exception. If not given, the error message returned by the server is used instead. bool Whether to set the cause attribute to the previous exception if the exception is translated. args, *kwargs : Additional arguments to pass to the exception constructor, after the error message. Useful for passing the filename arguments to IOError . Returns An instantiated exception ready to be thrown. If the error code isn't recognized, an IOError with the original error message is returned. Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/exceptions.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def translate_oss_error ( error : OssError , * args , message = None , set_cause = True , ** kwargs ) -> BaseException : \"\"\"Convert a ClientError exception into a Python one. Parameters ---------- error : oss2.exceptions.OssError The exception returned by the OSS Server. message : str An error message to use for the returned exception. If not given, the error message returned by the server is used instead. set_cause : bool Whether to set the __cause__ attribute to the previous exception if the exception is translated. *args, **kwargs : Additional arguments to pass to the exception constructor, after the error message. Useful for passing the filename arguments to ``IOError``. Returns ------- An instantiated exception ready to be thrown. If the error code isn't recognized, an IOError with the original error message is returned. \"\"\" if not isinstance ( error , OssError ): # not a oss error: return error code = error . __class__ . __name__ constructor = ERROR_CODE_TO_EXCEPTION . get ( code ) if constructor : if not message : message = error . message custom_exc = constructor ( message , * args , ** kwargs ) else : # No match found, wrap this in an IOError with the appropriate message. custom_exc = IOError ( errno . EIO , message or str ( error ), * args ) if set_cause : custom_exc . __cause__ = error return custom_exc","title":"translate_oss_error()"},{"location":"reference/ossfs/file/","text":"Code of OSSFileSystem and OSSFile OSSFile Bases: AbstractBufferedFile A file living in OSSFileSystem Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/file.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class OSSFile ( AbstractBufferedFile ): \"\"\"A file living in OSSFileSystem\"\"\" fs : Union [ \"OSSFileSystem\" , \"AioOSSFileSystem\" ] loc : int def _upload_chunk ( self , final : bool = False ) -> bool : \"\"\"Write one part of a multi-block file upload Parameters ========== final: bool This is the last block, so should complete file, if self.autocommit is True. \"\"\" self . loc = self . fs . append_object ( self . path , self . loc , self . buffer . getvalue ()) return True def _initiate_upload ( self ): \"\"\"Create remote file/upload\"\"\" if \"a\" in self . mode : self . loc = 0 if self . fs . exists ( self . path ): self . loc = self . fs . info ( self . path )[ \"size\" ] elif \"w\" in self . mode : # create empty file to append to self . loc = 0 if self . fs . exists ( self . path ): self . fs . rm_file ( self . path ) def _fetch_range ( self , start : int , end : int ) -> bytes : \"\"\" Get the specified set of bytes from remote Parameters ========== start: int end: int \"\"\" start = max ( start , 0 ) end = min ( self . size , end ) if start >= end or start >= self . size : return b \"\" return self . fs . get_object ( self . path , start , end )","title":"File"},{"location":"reference/ossfs/file/#ossfs.file.OSSFile","text":"Bases: AbstractBufferedFile A file living in OSSFileSystem Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/file.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class OSSFile ( AbstractBufferedFile ): \"\"\"A file living in OSSFileSystem\"\"\" fs : Union [ \"OSSFileSystem\" , \"AioOSSFileSystem\" ] loc : int def _upload_chunk ( self , final : bool = False ) -> bool : \"\"\"Write one part of a multi-block file upload Parameters ========== final: bool This is the last block, so should complete file, if self.autocommit is True. \"\"\" self . loc = self . fs . append_object ( self . path , self . loc , self . buffer . getvalue ()) return True def _initiate_upload ( self ): \"\"\"Create remote file/upload\"\"\" if \"a\" in self . mode : self . loc = 0 if self . fs . exists ( self . path ): self . loc = self . fs . info ( self . path )[ \"size\" ] elif \"w\" in self . mode : # create empty file to append to self . loc = 0 if self . fs . exists ( self . path ): self . fs . rm_file ( self . path ) def _fetch_range ( self , start : int , end : int ) -> bytes : \"\"\" Get the specified set of bytes from remote Parameters ========== start: int end: int \"\"\" start = max ( start , 0 ) end = min ( self . size , end ) if start >= end or start >= self . size : return b \"\" return self . fs . get_object ( self . path , start , end )","title":"OSSFile"},{"location":"reference/ossfs/utils/","text":"utils of ossfs as_progress_handler ( callback ) progress bar handler Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/utils.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def as_progress_handler ( callback ): \"\"\"progress bar handler\"\"\" if callback is None : return None sent_total = False def progress_handler ( absolute_progress , total_size ): nonlocal sent_total if not sent_total : callback . set_size ( total_size ) sent_total = True callback . absolute_update ( absolute_progress ) return progress_handler async_prettify_info_result ( func ) Make the return values of async func ls and info follows the fsspec's standard Examples: @async_pretify_info_result async def ls(path: str, ...) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/utils.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def async_prettify_info_result ( func ): \"\"\"Make the return values of async func `ls` and `info` follows the fsspec's standard Examples: -------------------------------- @async_pretify_info_result async def ls(path: str, ...) \"\"\" @wraps ( func ) async def wrapper ( ossfs , path : str , * args , ** kwargs ): func_params = inspect . signature ( func ) . parameters if \"detail\" in func_params : detail = kwargs . get ( \"detail\" , func_params [ \"detail\" ] . default ) else : detail = kwargs . get ( \"detail\" , True ) result = await func ( ossfs , path , * args , ** kwargs ) return _format_unify ( path , result , detail ) return wrapper prettify_info_result ( func ) Make the return values of ls and info follows the fsspec's standard Examples: @pretify_info_result def ls(path: str, ...) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/utils.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def prettify_info_result ( func ): \"\"\"Make the return values of `ls` and `info` follows the fsspec's standard Examples: -------------------------------- @pretify_info_result def ls(path: str, ...) \"\"\" @wraps ( func ) def wrapper ( ossfs , path : str , * args , ** kwargs ): func_params = inspect . signature ( func ) . parameters if \"detail\" in func_params : detail = kwargs . get ( \"detail\" , func_params [ \"detail\" ] . default ) else : detail = kwargs . get ( \"detail\" , True ) result = func ( ossfs , path , * args , ** kwargs ) return _format_unify ( path , result , detail ) return wrapper","title":"Utils"},{"location":"reference/ossfs/utils/#ossfs.utils.as_progress_handler","text":"progress bar handler Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/utils.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def as_progress_handler ( callback ): \"\"\"progress bar handler\"\"\" if callback is None : return None sent_total = False def progress_handler ( absolute_progress , total_size ): nonlocal sent_total if not sent_total : callback . set_size ( total_size ) sent_total = True callback . absolute_update ( absolute_progress ) return progress_handler","title":"as_progress_handler()"},{"location":"reference/ossfs/utils/#ossfs.utils.async_prettify_info_result","text":"Make the return values of async func ls and info follows the fsspec's standard Examples: @async_pretify_info_result async def ls(path: str, ...) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/utils.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def async_prettify_info_result ( func ): \"\"\"Make the return values of async func `ls` and `info` follows the fsspec's standard Examples: -------------------------------- @async_pretify_info_result async def ls(path: str, ...) \"\"\" @wraps ( func ) async def wrapper ( ossfs , path : str , * args , ** kwargs ): func_params = inspect . signature ( func ) . parameters if \"detail\" in func_params : detail = kwargs . get ( \"detail\" , func_params [ \"detail\" ] . default ) else : detail = kwargs . get ( \"detail\" , True ) result = await func ( ossfs , path , * args , ** kwargs ) return _format_unify ( path , result , detail ) return wrapper","title":"async_prettify_info_result()"},{"location":"reference/ossfs/utils/#ossfs.utils.prettify_info_result","text":"Make the return values of ls and info follows the fsspec's standard Examples: @pretify_info_result def ls(path: str, ...) Source code in /opt/hostedtoolcache/Python/3.10.11/x64/lib/python3.10/site-packages/ossfs/utils.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def prettify_info_result ( func ): \"\"\"Make the return values of `ls` and `info` follows the fsspec's standard Examples: -------------------------------- @pretify_info_result def ls(path: str, ...) \"\"\" @wraps ( func ) def wrapper ( ossfs , path : str , * args , ** kwargs ): func_params = inspect . signature ( func ) . parameters if \"detail\" in func_params : detail = kwargs . get ( \"detail\" , func_params [ \"detail\" ] . default ) else : detail = kwargs . get ( \"detail\" , True ) result = func ( ossfs , path , * args , ** kwargs ) return _format_unify ( path , result , detail ) return wrapper","title":"prettify_info_result()"}]}